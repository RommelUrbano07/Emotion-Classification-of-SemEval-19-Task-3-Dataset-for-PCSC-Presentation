{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM_3Class_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Qd5TJkehaWD7",
        "MssB-8729D65",
        "CDFx9-zp-dyt",
        "2uauCFeluZui",
        "VczXcS6Iv6Lt",
        "bqf9kHwDaWEK",
        "KnhqBGU9aWEQ",
        "xAlv9Y7WaWEc",
        "Rp8_oI_A91pO",
        "rpD7Lw0vdmvL"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHAj_wpfbpji"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd5TJkehaWD7"
      },
      "source": [
        "# ***Data/Library Import***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyU2wyAvaWEC"
      },
      "source": [
        "#library imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re \n",
        "import spacy\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import string\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import random\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34rtu6Q0aWEK",
        "outputId": "edf0a301-c6ca-4cd9-c2e1-72507c8a405c"
      },
      "source": [
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "#loading the train data by using the \"tab\" or \\t as a separator \n",
        "reviews = pd.read_csv(\"/content/drive/MyDrive/DL_Project_train.txt\",sep=\"\\t\")\n",
        "print(reviews.shape)\n",
        "reviews.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30160, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>turn1</th>\n",
              "      <th>turn2</th>\n",
              "      <th>turn3</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Don't worry  I'm girl</td>\n",
              "      <td>hmm how do I know if you are</td>\n",
              "      <td>What's ur name?</td>\n",
              "      <td>others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>When did I?</td>\n",
              "      <td>saw many times i think -_-</td>\n",
              "      <td>No. I never saw you</td>\n",
              "      <td>angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>By</td>\n",
              "      <td>by Google Chrome</td>\n",
              "      <td>Where you live</td>\n",
              "      <td>others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>U r ridiculous</td>\n",
              "      <td>I might be ridiculous but I am telling the truth.</td>\n",
              "      <td>U little disgusting whore</td>\n",
              "      <td>angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Just for time pass</td>\n",
              "      <td>wt do u do 4 a living then</td>\n",
              "      <td>Maybe</td>\n",
              "      <td>others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>I'm a dog person</td>\n",
              "      <td>youre so rude</td>\n",
              "      <td>Whaaaat why</td>\n",
              "      <td>others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>So whatsup</td>\n",
              "      <td>Nothing much. Sitting sipping and watching TV....</td>\n",
              "      <td>What are you watching on tv?</td>\n",
              "      <td>others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>Ok</td>\n",
              "      <td>ok im back!!</td>\n",
              "      <td>So, how are u</td>\n",
              "      <td>others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>Really?</td>\n",
              "      <td>really really really really really</td>\n",
              "      <td>Y saying so many times...i can hear you</td>\n",
              "      <td>others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>Bay</td>\n",
              "      <td>in the bay</td>\n",
              "      <td>😘 love you</td>\n",
              "      <td>others</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                  turn1  ...                                    turn3   label\n",
              "0   0  Don't worry  I'm girl  ...                          What's ur name?  others\n",
              "1   1            When did I?  ...                      No. I never saw you   angry\n",
              "2   2                     By  ...                           Where you live  others\n",
              "3   3         U r ridiculous  ...                U little disgusting whore   angry\n",
              "4   4     Just for time pass  ...                                    Maybe  others\n",
              "5   5       I'm a dog person  ...                              Whaaaat why  others\n",
              "6   6             So whatsup  ...             What are you watching on tv?  others\n",
              "7   7                     Ok  ...                            So, how are u  others\n",
              "8   8                Really?  ...  Y saying so many times...i can hear you  others\n",
              "9   9                    Bay  ...                               😘 love you  others\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua03TknGaWEK",
        "outputId": "e1ea911a-2dd6-436e-a063-3b1c8bbcfa42"
      },
      "source": [
        "#concatination of turn 1, turn 2, turn 3 to create a single column of text\n",
        "reviews['review'] = reviews['turn1'] + ' ' + reviews['turn2']+' '+reviews['turn3']\n",
        "\n",
        "#resetting the dataframe index caused by data partitioning\n",
        "reviews = reviews.sample(frac=1).reset_index(drop=True)\n",
        "print(reviews.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30160, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MssB-8729D65"
      },
      "source": [
        "# ***DATA ANALYSIS***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJegCSSN9OzP"
      },
      "source": [
        "dataset=reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "4LD0hm0i9S6O",
        "outputId": "6be609fb-0bbd-46fd-ecf4-517011611db9"
      },
      "source": [
        "#Visualization of the distribution on the train data\n",
        "print(dataset['label'].value_counts() , '\\n' )\n",
        "#Counting Target Variable Distribution\n",
        "print( (dataset['label'].value_counts()/len(dataset)) * 100 , '\\n' )\n",
        "#Counting Target Variable Distribution\n",
        "\n",
        "sns.displot(dataset['label'])\n",
        "#Seaborn Histogram Plot of Target Variable Distribution"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "others    14948\n",
            "angry      5506\n",
            "sad        5463\n",
            "happy      4243\n",
            "Name: label, dtype: int64 \n",
            "\n",
            "others    49.562334\n",
            "angry     18.255968\n",
            "sad       18.113395\n",
            "happy     14.068302\n",
            "Name: label, dtype: float64 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seaborn.axisgrid.FacetGrid at 0x7f59d21ea290>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_-ATMmt9xkm",
        "outputId": "226ca3cb-9f42-474a-a821-ef98714fbf63"
      },
      "source": [
        "#getting all the total unique words count of all the turns (turn 1, turn 2, turn 3)\n",
        "unique_words_all_turns=[]\n",
        "temp_list=dataset['turn1'].append(dataset['turn2'].append(dataset['turn3'])) #appending every turn into the variable in order to access each turns.\n",
        "#accessing the temp_list that contains the data for all turn and counts the total unique words.\n",
        "for word in temp_list:\n",
        "  temp = word.split(' ')\n",
        "  for word in temp:\n",
        "    if word not in unique_words_all_turns:\n",
        "      unique_words_all_turns.append(word)\n",
        "print('Unique word size for ALL TURNS:', len(unique_words_all_turns))\n",
        "#prints out the total frequency of all the unique words\n",
        "#word_count_value.append(len(unique_words_all_turns))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique word size for ALL TURNS: 33025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWKqzSfD905j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee0ecb9c-1d8f-4de1-d913-ed7011d73c9b"
      },
      "source": [
        "#this function simply outputs the unique word count/size of turn 1\n",
        "word_count_turns=[]\n",
        "temp_list=dataset['turn1'] #assigning the turn 1 data into a variable\n",
        "for word in temp_list: #the process on which every word is counted.\n",
        "  temp = word.split(' ')\n",
        "  for word in temp:\n",
        "    if word not in word_count_turns:\n",
        "      word_count_turns.append(word)\n",
        "print('Unique word size for FIRST TURN:', len(word_count_turns)) #prints out the unique word count of turn 1\n",
        "#word_count_value.append(len(word_count_turns))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique word size for FIRST TURN: 13146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR2tT_oy93Sr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc22ed1e-df1d-4f28-c418-4d8bdf0b10c5"
      },
      "source": [
        "#this function simply outputs the unique word count/size of turn 2\n",
        "word_count_turns=[]\n",
        "temp_list=dataset['turn2']  #assigning the turn 2 data into a variable\n",
        "for word in temp_list: #the process on which every word is counted.\n",
        "  temp = word.split(' ')\n",
        "  for word in temp:\n",
        "    if word not in word_count_turns:\n",
        "      word_count_turns.append(word)\n",
        "print('Unique word size for SECOND TURN:', len(word_count_turns)) #prints out the unique word count of turn 2\n",
        "#word_count_value.append(len(word_count_turns))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique word size for SECOND TURN: 19398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNaA83oU95Gb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b5b2b81-1233-48fc-8898-fe2ad3b1c429"
      },
      "source": [
        "#this function simply outputs the unique word count/size of turn 3\n",
        "word_count_turns=[]\n",
        "temp_list=dataset['turn3'] #assigning the turn 3 data into a variable\n",
        "for word in temp_list: #the process on which every word is counted.\n",
        "  temp = word.split(' ')\n",
        "  for word in temp:\n",
        "    if word not in word_count_turns:\n",
        "      word_count_turns.append(word)\n",
        "print('Unique word size for THIRD TURN:', len(word_count_turns)) #prints out the unique word count of turn 3\n",
        "#word_count_value.append(len(word_count_turns))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique word size for THIRD TURN: 12778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3smAIkDk95f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "f54a988f-d4b2-4ddb-e7dd-b0ef8e1e9602"
      },
      "source": [
        "#shows the visualization and comparison of the word frequency of turn 1, turn 2, and turn 3.\n",
        "import plotly as py\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "data = [go.Bar(\n",
        "            x = ['all','turn1','turn2','turn3'],\n",
        "            y = word_count_value,\n",
        "            marker = dict(colorscale = 'darkmint',\n",
        "                         color = word_count_value\n",
        "                         ),\n",
        "            text = 'Word counts'\n",
        "    )]\n",
        "\n",
        "layout = go.Layout(\n",
        "    title='Word Count Comparison (Turn 1, Turn 2, Turn 3)'\n",
        ")\n",
        "\n",
        "fig = go.Figure(data=data, layout = layout)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-0dcbf6adbd3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m data = [go.Bar(\n\u001b[1;32m      6\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'turn1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'turn2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'turn3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_count_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             marker = dict(colorscale = 'darkmint',\n\u001b[1;32m      9\u001b[0m                          \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_count_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'word_count_value' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V72R98yA96pg"
      },
      "source": [
        "# this funtion prints out and visualizes the top 10 unigrams for turn 1 (or the top 10 most used words in turn 1)\n",
        "words = dataset['turn1'].str.split(expand = True).unstack().value_counts() #accessing the dataset turn 1 and assigning it into a variable\n",
        "\n",
        "print(words.index.values[0:10],) #getting the top 10 words with the highest frequency\n",
        "print(words.values[0:10]) #getting the actual value or frequency of the top 10 unigrams.\n",
        "\n",
        "import plotly as py\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "#visualizes the top unigrams in descending order (starting from the frequently used words down to less used word) together with the actual count of each word.\n",
        "data = [go.Bar(\n",
        "            x = words.index.values[0:10],\n",
        "            y = words.values[0:10],\n",
        "            marker = dict(colorscale = 'magma',\n",
        "                         color = words.values[0:160]\n",
        "                         ),\n",
        "            text = 'Word counts'\n",
        "    )]\n",
        "\n",
        "layout = go.Layout(\n",
        "    title='Top 10 Words (Turn 1)'\n",
        ")\n",
        "\n",
        "fig = go.Figure(data=data, layout = layout)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rem4i0jF978j"
      },
      "source": [
        "# this funtion prints out and visualizes the top 10 unigrams for turn 2 (or the top 10 most used words in turn 2)\n",
        "words = dataset['turn2'].str.split(expand = True).unstack().value_counts() #accessing the dataset turn 2 and assigning it into a variable\n",
        "\n",
        "\n",
        "print(words.index.values[0:10],)#getting the top 10 words with the highest frequency\n",
        "print(words.values[0:10]) #getting the actual value or frequency of the top 10 unigrams.\n",
        "\n",
        "#visualizes the top unigrams in descending order (starting from the frequently used words down to less used word) together with the actual count of each word.\n",
        "import plotly as py\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "data = [go.Bar(\n",
        "            x = words.index.values[0:10],\n",
        "            y = words.values[0:10],\n",
        "            marker = dict(colorscale = 'magenta',\n",
        "                         color = words.values[0:160]\n",
        "                         ),\n",
        "            text = 'Word counts'\n",
        "    )]\n",
        "\n",
        "layout = go.Layout(\n",
        "    title='Top 10 Words (Turn 2)'\n",
        ")\n",
        "\n",
        "fig = go.Figure(data=data, layout = layout)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za4ypufq99gM"
      },
      "source": [
        "# this funtion prints out and visualizes the top 10 unigrams for turn 3 (or the top 10 most used words in turn 3)\n",
        "words = dataset['turn3'].str.split(expand = True).unstack().value_counts() #accessing the dataset turn 3 and assigning it into a variable\n",
        "\n",
        "\n",
        "print(words.index.values[0:10],) #getting the top 10 words with the highest frequency\n",
        "print(words.values[0:10]) #getting the actual value or frequency of the top 10 unigrams.\n",
        "#visualizes the top unigrams in descending order (starting from the frequently used words down to less used word) together with the actual count of each word.\n",
        "import plotly as py\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "data = [go.Bar(\n",
        "            x = words.index.values[0:10],\n",
        "            y = words.values[0:10],\n",
        "            marker = dict(colorscale = 'rainbow',\n",
        "                         color = words.values[0:160]\n",
        "                         ),\n",
        "            text = 'Word counts'\n",
        "    )]\n",
        "\n",
        "layout = go.Layout(\n",
        "    title='Top 10 Words (Turn 3)'\n",
        ")\n",
        "\n",
        "fig = go.Figure(data=data, layout = layout)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BypVqeq79_U5"
      },
      "source": [
        "## this funtion prints out and visualizes the top 20 unigrams for all turns (or the top 20 most used words in all turns)\n",
        "words = dataset['turn1'].append(dataset['turn2'].append(dataset['turn3'])).str.split(expand = True).unstack().value_counts() #accessing the dataset of all turns and assign it into a variable\n",
        "\n",
        "print(words.index.values[0:20]) #getting the top 20 words with the highest frequency\n",
        "print(words.values[0:20]) #getting the actual value or frequency of the top 20 unigrams.\n",
        "#visualizes the top unigrams in descending order (starting from the frequently used words down to less used word) together with the actual count of each word.\n",
        "import plotly as py\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "data = [go.Bar(\n",
        "            x = words.index.values[0:20],\n",
        "            y = words.values[0:20],\n",
        "            marker = dict(colorscale = 'darkmint',\n",
        "                         color = words.values[0:160]\n",
        "                         ),\n",
        "            text = 'Word counts'\n",
        "    )]\n",
        "\n",
        "layout = go.Layout(\n",
        "    title='Top 20 Words (All Turns)'\n",
        ")\n",
        "\n",
        "fig = go.Figure(data=data, layout = layout)\n",
        "fig.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Cdba8fo-Aez"
      },
      "source": [
        "#organizing the dataset depending on the label (happy, sad, angry, or others)\n",
        "df = dataset.groupby('label') \n",
        "happy = df.get_group('happy')\n",
        "sad = df.get_group('sad')\n",
        "angry = df.get_group('angry')\n",
        "others = df.get_group('others')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9OjKb72-GQH"
      },
      "source": [
        "#counts the overall data under the \"Happy Class\" in turn 1\n",
        "value1=0\n",
        "temp_list=happy['turn1']\n",
        "for word in temp_list:\n",
        "  value1+=1\n",
        "print('Records for Happy Class:', value1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVDQWqdB-Gp4"
      },
      "source": [
        "#counts the overall data under the \"Sad Class\" in turn 1\n",
        "value2=0\n",
        "temp_list=sad['turn1']\n",
        "for word in temp_list:\n",
        "  value2+=1\n",
        "print('Records for Sad Class:', value2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URobL5Zo-Gzx"
      },
      "source": [
        "#counts the overall data under the \"Angry Class\" in turn 1\n",
        "value3=0\n",
        "temp_list=angry['turn1']\n",
        "for word in temp_list:\n",
        "  value3+=1\n",
        "print('Records for Angry Class:', value3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxPEFgyT-G2y"
      },
      "source": [
        "#counts the overall data under the \"Others Class\" in turn 1\n",
        "value4=0\n",
        "temp_list=others['turn1']\n",
        "for word in temp_list:\n",
        "  value4+=1\n",
        "print('Records for Others Class:', value4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j_QeTeZ-Kwb"
      },
      "source": [
        "#function on getting the top 20 words in under the \"Happy Class\" on all turn (turn 1, turn 2, turn 3)\n",
        "words = happy['turn1'].append(happy['turn2'].append(happy['turn3'])).str.split(expand = True).unstack().value_counts()#accessing all turns\n",
        "\n",
        "print(words.index.values[0:20])#getting the top 20 words with the highest frequency\n",
        "print(words.values[0:20]) #getting the actual value or frequency of the top 20 unigrams.\n",
        "\n",
        "#visualizes the top unigrams in descending order (starting from the frequently used words down to less used word) together with the actual count of each word.\n",
        "import plotly as py\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "data = [go.Bar(\n",
        "            x = words.index.values[0:20],\n",
        "            y = words.values[0:20],\n",
        "            marker = dict(colorscale = 'darkmint',\n",
        "                         color = words.values[0:160]\n",
        "                         ),\n",
        "            text = 'Word counts'\n",
        "    )]\n",
        "\n",
        "layout = go.Layout(\n",
        "    title='Top 20 Words on Happy Class (All Turns)'\n",
        ")\n",
        "\n",
        "fig = go.Figure(data=data, layout = layout)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGJnFaLM-K49"
      },
      "source": [
        "#function on getting the top 20 words in under the \"Sad Class\" on all turn (turn 1, turn 2, turn 3)\n",
        "words = sad['turn1'].append(sad['turn2'].append(sad['turn3'])).str.split(expand = True).unstack().value_counts() #accessing all turns\n",
        "\n",
        "print(words.index.values[0:20])#getting the top 20 words with the highest frequency\n",
        "print(words.values[0:20]) #getting the actual value or frequency of the top 20 unigrams.\n",
        "\n",
        "#visualizes the top unigrams in descending order (starting from the frequently used words down to less used word) together with the actual count of each word.\n",
        "\n",
        "import plotly as py\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "data = [go.Bar(\n",
        "            x = words.index.values[0:20],\n",
        "            y = words.values[0:20],\n",
        "            marker = dict(colorscale = 'darkmint',\n",
        "                         color = words.values[0:160]\n",
        "                         ),\n",
        "            text = 'Word counts'\n",
        "    )]\n",
        "\n",
        "layout = go.Layout(\n",
        "    title='Top 20 Words on Sad Class (All Turns)'\n",
        ")\n",
        "\n",
        "fig = go.Figure(data=data, layout = layout)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KAhrXIs-N32"
      },
      "source": [
        "#function on getting the top 20 words in under the \"Angry Class\" on all turn (turn 1, turn 2, turn 3)\n",
        "words = angry['turn1'].append(angry['turn2'].append(angry['turn3'])).str.split(expand = True).unstack().value_counts() #accessing all turns\n",
        "\n",
        "print(words.index.values[0:20])#getting the top 20 words with the highest frequency\n",
        "print(words.values[0:20]) #getting the actual value or frequency of the top 20 unigrams.\n",
        "\n",
        "#visualizes the top unigrams in descending order (starting from the frequently used words down to less used word) together with the actual count of each word.\n",
        "import plotly as py\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "data = [go.Bar(\n",
        "            x = words.index.values[0:20],\n",
        "            y = words.values[0:20],\n",
        "            marker = dict(colorscale = 'darkmint',\n",
        "                         color = words.values[0:160]\n",
        "                         ),\n",
        "            text = 'Word counts'\n",
        "    )]\n",
        "\n",
        "layout = go.Layout(\n",
        "    title='Top 20 Words on Angry Class (All Turns)'\n",
        ")\n",
        "\n",
        "fig = go.Figure(data=data, layout = layout)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zh4Znww-PEt"
      },
      "source": [
        "#function on getting the top 20 words in under the \"Others Class\" on all turn (turn 1, turn 2, turn 3)\n",
        "words = others['turn1'].append(others['turn2'].append(others['turn3'])).str.split(expand = True).unstack().value_counts()#accessing all turns\n",
        "\n",
        "print(words.index.values[0:20])#getting the top 20 words with the highest frequency\n",
        "print(words.values[0:20]) #getting the actual value or frequency of the top 20 unigrams.\n",
        "\n",
        "#visualizes the top unigrams in descending order (starting from the frequently used words down to less used word) together with the actual count of each word.\n",
        "\n",
        "import plotly as py\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "data = [go.Bar(\n",
        "            x = words.index.values[0:20],\n",
        "            y = words.values[0:20],\n",
        "            marker = dict(colorscale = 'darkmint',\n",
        "                         color = words.values[0:160]\n",
        "                         ),\n",
        "            text = 'Word counts'\n",
        "    )]\n",
        "\n",
        "layout = go.Layout(\n",
        "    title='Top 20 Words on Others Class (All Turns)'\n",
        ")\n",
        "\n",
        "fig = go.Figure(data=data, layout = layout)\n",
        "fig.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Srq9L5lY-Rb0"
      },
      "source": [
        "#INSTALLING ADVERTOOLS - productivity and analysis tools used to analyze text and content\n",
        "pip install advertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1OrfmMa-Rmw"
      },
      "source": [
        "#Advertools is used to extract and analyze the emojis in the data\n",
        "import advertools as adv\n",
        "emoji_summary = adv.extract_emoji(dataset['turn1'].append(dataset['turn2'].append(dataset['turn3'])))\n",
        "#List of dictionary keys such as ['emoji', 'emoji_text', 'emoji_flat', 'emoji_flat_text',\n",
        "#'emoji_counts', 'emoji_freq', 'top_emoji', 'top_emoji_text', 'top_emoji_groups', 'top_emoji_sub_groups', 'overview']\n",
        "#These keys are used to analyze the emojis in your data\n",
        "emoji_summary.keys()\n",
        "#Shows the distribution of all emojis in the data in a descending order (emoji, frequency) \n",
        "emoji_summary['top_emoji']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av0n1qIp-RyZ"
      },
      "source": [
        "#Overview of the emojis in the dataset\n",
        "emoji_summary['overview']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1zXR1CK-R0W"
      },
      "source": [
        "#Almost the same with 'top_emoji' but this time the emoji name is included (emoji, emoji name, frequency) and it is still in descending order\n",
        "for emoji, text in (zip([x[0] for x in emoji_summary['top_emoji']], \n",
        "emoji_summary['top_emoji_text'], )):\n",
        "  print(emoji,*text, sep=' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-mEYQpd-R5P"
      },
      "source": [
        "#Visualization of the distribution of the top 20 emoji regardless of the emotion class, it shows the emoji name along with its frequency.\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(facecolor='#eeeeee')\n",
        "fig.set_size_inches((9, 9))\n",
        "ax.set_frame_on(False)\n",
        "ax.barh([x[0] for x in emoji_summary['top_emoji_text'][:20]][::-1],\n",
        "[x[1] for x in emoji_summary['top_emoji_text'][:20]][::-1])\n",
        "ax.tick_params(labelsize=14)\n",
        "ax.set_title('Top 20 Emoji', fontsize=18)\n",
        "ax.grid()\n",
        "fig.savefig(ax.get_title() + '.png', \n",
        "facecolor='#eeeeee',dpi=120,\n",
        "bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiUDIUuUgWop"
      },
      "source": [
        "**Top Emojis Per Emotion Class**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpz6ovfLhoLc"
      },
      "source": [
        "#Grouping the dataset according to each label/emotion class (Happy, Sad, Angry, and Others)\n",
        "#In order to extract the top emojis per emotion class\n",
        "df = dataset.groupby('label') \n",
        "happy = df.get_group('happy')\n",
        "sad = df.get_group('sad')\n",
        "angry = df.get_group('angry')\n",
        "others = df.get_group('others')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn1k3SsI-R7i"
      },
      "source": [
        "#Happy Emotion Class\n",
        "print('Top Emojis (Happy Emotion Class)')\n",
        "happy_summary = adv.extract_emoji(happy['turn1'].append(happy['turn2'].append(happy['turn3'])))\n",
        "happy_summary['top_emoji']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCRU6IZL-R9Z"
      },
      "source": [
        "#Sad Emotion Class\n",
        "print('Top Emojis (Sad Emotion Class)')\n",
        "sad_summary = adv.extract_emoji(sad['turn1'].append(sad['turn2'].append(sad['turn3'])))\n",
        "sad_summary['top_emoji']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lamSxh-i-aNV"
      },
      "source": [
        "#Angry Emotion Class\n",
        "print('Top Emojis (Angry Emotion Class)')\n",
        "angry_summary = adv.extract_emoji(angry['turn1'].append(angry['turn2'].append(angry['turn3'])))\n",
        "angry_summary['top_emoji']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1lAi9Up-R_a"
      },
      "source": [
        "#Others Emotion Class\n",
        "print('Top Emojis (Others)')\n",
        "others_summary = adv.extract_emoji(others['turn1'].append(others['turn2'].append(others['turn3'])))\n",
        "others_summary['top_emoji']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDFx9-zp-dyt"
      },
      "source": [
        "# ***DATA CLEANING METHODS***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjzUBugu-f7Z"
      },
      "source": [
        "#DATA PRE-PROCESSING\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "from string import punctuation\n",
        "\n",
        "#correcting some mispelled stop words suchs as u to you, r to are, and y to why\n",
        "def correct_u_r_y(turn_text):\n",
        "  temp=turn_text.split(\" \")\n",
        "  for i in range (0,len(temp)):\n",
        "    temp[i]=re.sub('^[U]$','You',temp[i])\n",
        "    temp[i]=re.sub('^[u]$','you',temp[i])\n",
        "    temp[i]=re.sub('^[R]$','Are',temp[i])\n",
        "    temp[i]=re.sub('^[r]$','are',temp[i])\n",
        "    temp[i]=re.sub('^[Y]$','Why',temp[i]) \n",
        "    temp[i]=re.sub('^[y]$','why',temp[i])\n",
        "    temp[i]=re.sub('^(Ur)$','Your',temp[i])\n",
        "    temp[i]=re.sub('^(ur)$','your',temp[i])\n",
        "    temp[i]=re.sub('^(UR)$','YOUR',temp[i])\n",
        "    # temp[i] = temp[i].str.replace(\"\\s{2,}\", \" \")\n",
        "  filtered_sentence = (\" \").join(temp)\n",
        "  return filtered_sentence\n",
        "\n",
        "#correcting all mispelled words using TextBlob\n",
        "def correct_spelling(turn_text):\n",
        "  temp=turn_text.split(\" \")\n",
        "  for i in range (0,len(temp)):\n",
        "    textBlb = TextBlob(temp[i])            \n",
        "    textCorrected = textBlb.correct()\n",
        "    temp[i]=str(textCorrected)\n",
        "  filtered_sentence = (\" \").join(temp)\n",
        "  return filtered_sentence\n",
        "\n",
        "#replacing repeated close parenthesis into their single instances :))) -> :) )\n",
        "def reduce_characters(turn_text):\n",
        "  charList=set(punctuation)\n",
        "  for i in charList:\n",
        "    regex=\"(\\\\\" +i+\"\\\\\"+i+\"+)\"\n",
        "    turn_text=re.sub( regex, \"\\\\\"+i, turn_text)\n",
        "  return turn_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uezSt_Q8-pt_"
      },
      "source": [
        "#DOWNLOADING THE STOP WORDS using NLTK\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "#STOP WORDS REMOVAL - based on the list of stop words provided by NLTK\n",
        "def stop_words_removal(turn_text):\n",
        "  turn_text= turn_text.lower().split(\" \")\n",
        "  turn_text = [word for word in turn_text if word not in stop]\n",
        "  filtered_sentence = (\" \").join(turn_text)\n",
        "  return filtered_sentence\n",
        "\n",
        "#STEMMING - reducing the word to its root form\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "def stemming_words(turn_text):\n",
        "  stemmer = SnowballStemmer(language='english')\n",
        "  turn_text= stemmer.stem(turn_text)\n",
        "  return turn_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjlVdjkx-q9Q"
      },
      "source": [
        "#DOWNLOADING WORDNET using NLTK - English lexical database\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "#LOWER CASING STRINGS\n",
        "def lower_case (turn_text):\n",
        "  string = turn_text\n",
        "  return string.lower()\n",
        "\n",
        "#LEMMATIZATION - is like stemming but looks at the meaning of the word rather than the form of the word\n",
        "def lemmatization(turn_text):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  return lemmatizer.lemmatize(turn_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqv66kin-sUz"
      },
      "source": [
        "#All the data pre-processing methods are in a sequential order (depending on its use) combined into one function called 'cleaning'\n",
        "def cleaning(turn):\n",
        "  # turn=turn.apply(lambda x : [reduce_characters(item) for item in x])\n",
        "  # print(1)\n",
        "  turn=turn.apply(lambda x : [lower_case(item) for item in x])\n",
        "  print(2)\n",
        "  # turn=turn.apply(lambda x : [stemming_words(item) for item in x])\n",
        "  # print(3)\n",
        "  # turn=turn.apply(lambda x : [correct_u_r_y(item) for item in x])\n",
        "  # print(4)\n",
        "  # turn=turn.apply(lambda x : [correct_spelling(item) for item in x])\n",
        "  # print(5)\n",
        "  turn=turn.apply(lambda x : [stop_words_removal(item) for item in x])\n",
        "  print(6)\n",
        "  # turn=turn.apply(lambda x : [lemmatization(item) for item in x])\n",
        "  # print(7)\n",
        "  return turn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uauCFeluZui"
      },
      "source": [
        "# ***RUN THIS TO CLEAN DATA, ELSE LEAVE ALONE***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV1BjMPouZcW"
      },
      "source": [
        "reviews['review']=cleaning(reviews['review'].to_frame())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VczXcS6Iv6Lt"
      },
      "source": [
        "# ***Create Dataset 1-3***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpGk7eNQv_z9",
        "outputId": "8137364d-8b5a-47ee-f504-6d4cd7e6374c"
      },
      "source": [
        "others=reviews[reviews['label']==\"others\"]\n",
        "happy=reviews[reviews['label']==\"happy\"]\n",
        "angry=reviews[reviews['label']==\"angry\"]\n",
        "sad=reviews[reviews['label']==\"sad\"]\n",
        "print(len(others))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "WcC2VfqFyfNN",
        "outputId": "5483b1c8-7263-418e-d217-582762d33927"
      },
      "source": [
        "dataset1=pd.concat([happy,angry,sad,others[0:4982]])\n",
        "dataset2=pd.concat([happy,angry,sad,others[4982:9964]])\n",
        "dataset3=pd.concat([happy,angry,sad,others[9964:]])\n",
        "\n",
        "dataset1.sample(frac=1).reset_index(drop=True)\n",
        "dataset2.sample(frac=1).reset_index(drop=True)\n",
        "dataset3.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>turn1</th>\n",
              "      <th>turn2</th>\n",
              "      <th>turn3</th>\n",
              "      <th>label</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>500</td>\n",
              "      <td>So sad</td>\n",
              "      <td>it's a sad sad situation</td>\n",
              "      <td>I am alone</td>\n",
              "      <td>sad</td>\n",
              "      <td>sad sad sad situation alone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>674</td>\n",
              "      <td>Why??</td>\n",
              "      <td>Always you are judging me. :'( But I &lt;3 you ra.</td>\n",
              "      <td>Wow lv u to??</td>\n",
              "      <td>happy</td>\n",
              "      <td>why?? always judging me. :'( &lt;3 ra. wow lv u to??</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>23602</td>\n",
              "      <td>I don't like chocolate</td>\n",
              "      <td>WASTE?!?!?! I love chocolate sandwich! Ever tr...</td>\n",
              "      <td>I hate them... I hate you</td>\n",
              "      <td>angry</td>\n",
              "      <td>like chocolate waste?!?!?! love chocolate sand...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20182</td>\n",
              "      <td>OK dear really</td>\n",
              "      <td>You're welcome. 👍</td>\n",
              "      <td>I like it</td>\n",
              "      <td>happy</td>\n",
              "      <td>ok dear really  welcome. 👍 like</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7429</td>\n",
              "      <td>Are you dumb?</td>\n",
              "      <td>so as you</td>\n",
              "      <td>😠😠</td>\n",
              "      <td>angry</td>\n",
              "      <td>dumb? 😠😠</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20191</th>\n",
              "      <td>24375</td>\n",
              "      <td>yes</td>\n",
              "      <td>ID DANCE AWAY FROM THEM</td>\n",
              "      <td>not  dance sexy chat</td>\n",
              "      <td>others</td>\n",
              "      <td>yes id dance away  dance sexy chat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20192</th>\n",
              "      <td>23760</td>\n",
              "      <td>In India or out of India</td>\n",
              "      <td>Hi, Please refer to my answer to What are some...</td>\n",
              "      <td>AgrA</td>\n",
              "      <td>others</td>\n",
              "      <td>india india hi, please refer answer travel des...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20193</th>\n",
              "      <td>21905</td>\n",
              "      <td>Who is rude</td>\n",
              "      <td>I'm not being rude</td>\n",
              "      <td>Ok by</td>\n",
              "      <td>angry</td>\n",
              "      <td>rude i'm rude ok</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20194</th>\n",
              "      <td>23450</td>\n",
              "      <td>Legends of tomorrow</td>\n",
              "      <td>best of luck for tomorrow</td>\n",
              "      <td>Riverdale</td>\n",
              "      <td>others</td>\n",
              "      <td>legends tomorrow best luck tomorrow riverdale</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20195</th>\n",
              "      <td>22660</td>\n",
              "      <td>Why not nice</td>\n",
              "      <td>you don't speak because you're rude! 😾</td>\n",
              "      <td>You are rude</td>\n",
              "      <td>angry</td>\n",
              "      <td>nice speak rude! 😾 rude</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20196 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                             review\n",
              "0        500  ...                        sad sad sad situation alone\n",
              "1        674  ...  why?? always judging me. :'( <3 ra. wow lv u to??\n",
              "2      23602  ...  like chocolate waste?!?!?! love chocolate sand...\n",
              "3      20182  ...                   ok dear really  welcome. 👍 like \n",
              "4       7429  ...                                           dumb? 😠😠\n",
              "...      ...  ...                                                ...\n",
              "20191  24375  ...                 yes id dance away  dance sexy chat\n",
              "20192  23760  ...  india india hi, please refer answer travel des...\n",
              "20193  21905  ...                                   rude i'm rude ok\n",
              "20194  23450  ...      legends tomorrow best luck tomorrow riverdale\n",
              "20195  22660  ...                            nice speak rude! 😾 rude\n",
              "\n",
              "[20196 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1KhJimq6cp3",
        "outputId": "2fe4f64c-1509-4643-f689-d55898da0c23"
      },
      "source": [
        "print(len(dataset1[dataset1.label==\"others\"]))\n",
        "print(len(dataset2[dataset2.label==\"others\"]))\n",
        "print(len(dataset3[dataset3.label==\"others\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4982\n",
            "4982\n",
            "4984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fet3AcI9BQA"
      },
      "source": [
        "# ***SELECT WHICH DATASET TO USE***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaY0issf9FkX"
      },
      "source": [
        "# reviews=dataset1\n",
        "reviews=dataset2\n",
        "# reviews=dataset3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqf9kHwDaWEK"
      },
      "source": [
        "# ***Data Partitioning and Pre-Processing***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "RuWxDA8XaWEL",
        "outputId": "255ebeba-06b8-4d03-91c2-531e6cc34960"
      },
      "source": [
        "#getting the text (review) and the emotion label (rating)\n",
        "reviews = reviews[['review', 'label']]\n",
        "reviews.columns = ['review', 'rating']\n",
        "# calculating sentence lengths\n",
        "reviews['review_length'] = reviews['review'].apply(lambda x: len(x.split()))\n",
        "reviews.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>rating</th>\n",
              "      <th>review_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>today news whats good news 😂</td>\n",
              "      <td>happy</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>really get married  get married. parents happy...</td>\n",
              "      <td>happy</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>street cat named bob meow... ;-) 😂</td>\n",
              "      <td>happy</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>u r sweetest &lt;3 thanks sweet ☺ 😘😘😘</td>\n",
              "      <td>happy</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>dragon ball z dragon ball gt super</td>\n",
              "      <td>happy</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               review rating  review_length\n",
              "1                        today news whats good news 😂  happy              6\n",
              "4   really get married  get married. parents happy...  happy             10\n",
              "17                 street cat named bob meow... ;-) 😂  happy              7\n",
              "18                 u r sweetest <3 thanks sweet ☺ 😘😘😘  happy              8\n",
              "30                 dragon ball z dragon ball gt super  happy              7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2IAdwkMaWEL",
        "outputId": "81907053-4b90-4fe8-9a09-1fbe9302faed"
      },
      "source": [
        "#convertion of the classes into numerical format\n",
        "zero_numbering = {'happy':0, 'sad':1, 'angry':2,\"others\":3}\n",
        "reviews['rating'] = reviews['rating'].apply(lambda x: zero_numbering[x])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyDDQgv0aWEL",
        "outputId": "6fc27761-f4ed-4ae9-d99d-a506dd1107ab"
      },
      "source": [
        "#mean sentence length\n",
        "np.mean(reviews['review_length'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.5752624282036045"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmPYcyo2aWEM"
      },
      "source": [
        "#tokenization\n",
        "tok = spacy.load('en')\n",
        "def tokenize (text):\n",
        "    return [token.text for token in tok.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYBQL9j5aWEN"
      },
      "source": [
        "#count number of occurences of each word\n",
        "counts = Counter()\n",
        "for index, row in reviews.iterrows():\n",
        "    counts.update(tokenize(row['review']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzZYr1WmaWEN"
      },
      "source": [
        "#creating vocabulary\n",
        "vocab2index = {\"\":0, \"UNK\":1}\n",
        "words = [\"\", \"UNK\"]\n",
        "for word in counts:\n",
        "    vocab2index[word] = len(words)\n",
        "    words.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK4irekoaWEO"
      },
      "source": [
        "#sentence encoding where the creation of the vocab2index\n",
        "#format Word:Number of Occurence i.e: You:100\n",
        "#and word list is made\n",
        "def encode_sentence(text, vocab2index, N=70):\n",
        "    tokenized = tokenize(text)\n",
        "    encoded = np.zeros(N, dtype=int)\n",
        "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
        "    length = min(N, len(enc1))\n",
        "    encoded[:length] = enc1[:length]\n",
        "    return encoded, length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "MsYsghDmaWEO",
        "outputId": "02ca4f1f-b719-443b-cc2b-9525d124c4dd"
      },
      "source": [
        "#application of the encode_sentence method\n",
        "reviews['encoded'] = reviews['review'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\n",
        "reviews.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>rating</th>\n",
              "      <th>review_length</th>\n",
              "      <th>encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>today news whats good news 😂</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>[[2, 3, 4, 5, 6, 3, 7, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>really get married  get married. parents happy...</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>[[8, 9, 10, 11, 9, 10, 12, 13, 14, 14, 12, 15,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>street cat named bob meow... ;-) 😂</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>[[17, 18, 19, 20, 21, 22, 23, 7, 0, 0, 0, 0, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>u r sweetest &lt;3 thanks sweet ☺ 😘😘😘</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>[[24, 25, 26, 27, 28, 29, 30, 31, 31, 31, 0, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>dragon ball z dragon ball gt super</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>[[32, 33, 34, 32, 33, 35, 36, 0, 0, 0, 0, 0, 0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               review  ...                                            encoded\n",
              "1                        today news whats good news 😂  ...  [[2, 3, 4, 5, 6, 3, 7, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
              "4   really get married  get married. parents happy...  ...  [[8, 9, 10, 11, 9, 10, 12, 13, 14, 14, 12, 15,...\n",
              "17                 street cat named bob meow... ;-) 😂  ...  [[17, 18, 19, 20, 21, 22, 23, 7, 0, 0, 0, 0, 0...\n",
              "18                 u r sweetest <3 thanks sweet ☺ 😘😘😘  ...  [[24, 25, 26, 27, 28, 29, 30, 31, 31, 31, 0, 0...\n",
              "30                 dragon ball z dragon ball gt super  ...  [[32, 33, 34, 32, 33, 35, 36, 0, 0, 0, 0, 0, 0...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9xrjmt9aWEP",
        "outputId": "71a402fb-c340-487b-d749-1d4122525170"
      },
      "source": [
        "#Checking the data distribution for each class\n",
        "Counter(reviews['rating'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 4243, 1: 5463, 2: 5506, 3: 4984})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4cWVJftaWEP"
      },
      "source": [
        "#Splitting the Train and Validation Set to be used during processing\n",
        "#Validation Set will consist of 20% of the data, and 80% for the Train Set\n",
        "X = list(reviews['encoded'])\n",
        "y = list(reviews['rating'])\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnhqBGU9aWEQ"
      },
      "source": [
        "# ***Pytorch Dataset***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMnB9fqj9K5w"
      },
      "source": [
        "#Checking whether GPU exists, if it does, use GPU instead of default CPU\n",
        "#for LSTM model processing\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do-x54TkaWEQ"
      },
      "source": [
        "#Convertion of the Pandas Dataframe into a Pytorch Dataset\n",
        "class ReviewsDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.y = Y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx][0].astype(np.int32)).to(device), self.y[idx], self.X[idx][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eqIWSgvaWER"
      },
      "source": [
        "#Application of the Pytorch Dataset on the Pandas DataFrame format\n",
        "train_ds = ReviewsDataset(X_train, y_train)\n",
        "valid_ds = ReviewsDataset(X_valid, y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yalPTcdEaWES"
      },
      "source": [
        "#Creating 64 batches of the train and validation dataset\n",
        "#using the DataLoader library\n",
        "batch_size = 64\n",
        "vocab_size = len(words)\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size)\n",
        "val_dl = DataLoader(valid_ds, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAlv9Y7WaWEc"
      },
      "source": [
        "# ***LSTM with pretrained GloVe word embedding training***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHS5PrUDaWER"
      },
      "source": [
        "def train_model(model, epochs, lr,saving_path):\n",
        "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    #Utilization of Stochastic Gradient Descent for optimization\n",
        "    #with corresponding Learning Rate\n",
        "    best_valid_loss = float('inf')\n",
        "    optimizer = torch.optim.SGD(parameters, lr=lr)\n",
        "    best_epoch=0\n",
        "    for i in range(epochs):\n",
        "        sum_loss = 0.0\n",
        "        total = 0\n",
        "        correct=0.0\n",
        "        for x, y, l in train_dl:\n",
        "            # x is the data\n",
        "            # y is the target variable (true label)\n",
        "            # l is the label\n",
        "\n",
        "            x = x.long()\n",
        "            y = y.long().to(device)\n",
        "\n",
        "            #prediction of the model using the given data\n",
        "            y_pred = model(x, l)\n",
        "            pred = torch.max(y_pred, 1)[1]\n",
        "\n",
        "            #resetting gradients for each iteration\n",
        "            optimizer.zero_grad()\n",
        "            #using Cross Entropy for Loss calculation\n",
        "            loss = F.cross_entropy(y_pred, y).to(device)\n",
        "            #backward propagation\n",
        "            loss.backward()\n",
        "            #gradient step\n",
        "            optimizer.step()\n",
        "            #calculation of total loss value\n",
        "            sum_loss += loss.item()*y.shape[0]\n",
        "            total += y.shape[0]\n",
        "            correct += (pred == y).float().sum()\n",
        "        #return values from the validation metrics    \n",
        "        val_loss, val_acc, val_rmse = validation_metrics(model, val_dl)\n",
        "\n",
        "        #gathering the data for train loss, validation loss, accuracy\n",
        "        #for each epoch\n",
        "        train_loss.append(sum_loss/total)\n",
        "        validation_loss.append(val_loss)\n",
        "        accuracy_val.append(val_acc)\n",
        "\n",
        "        print(\"Epoch \"+str(i+1)+\": train loss %.3f, val loss %.3f, train accuracy %.3f, val accuracy %.3f, and val rmse %.3f\" % \n",
        "              (sum_loss/total, val_loss, correct/total, val_acc, val_rmse))\n",
        "        \n",
        "        if val_loss < best_valid_loss:\n",
        "          best_valid_loss = val_loss\n",
        "          best_val_acc=val_acc\n",
        "          best_epoch=i\n",
        "          torch.save({'epoch': i,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': val_loss,\n",
        "            'train_loss': sum_loss/total,\n",
        "            'train_acc': correct/total,\n",
        "            'val_acc':val_acc}\n",
        "            , saving_path)\n",
        "          \n",
        "\n",
        "        if abs((sum_loss/total)-val_loss) >= 0.2:\n",
        "          return best_epoch\n",
        "    return best_epoch\n",
        "\n",
        "def validation_metrics (model, valid_dl):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    sum_loss = 0.0\n",
        "    sum_rmse = 0.0\n",
        "    for x, y, l in valid_dl:\n",
        "      # x is the data\n",
        "      # y is the target variable (true label)\n",
        "      # l is the label\n",
        "      x = x.long()\n",
        "      y = y.long().to(device)\n",
        "\n",
        "      #prediction of the model using the given data\n",
        "      y_hat = model(x, l)\n",
        "\n",
        "      #using Cross Entropy for Loss calculation\n",
        "      loss = F.cross_entropy(y_hat, y).to(device)\n",
        "      pred = torch.max(y_hat, 1)[1]\n",
        "\n",
        "      #collecting the prediction data for y_pred use in classification report\n",
        "      prediction.append(pred)\n",
        "\n",
        "      #calculation of total loss value and RMSE\n",
        "      correct += (pred == y).float().sum()\n",
        "      total += y.shape[0]\n",
        "      sum_loss += loss.item()*y.shape[0]\n",
        "      sum_rmse += np.sqrt(mean_squared_error(pred.cpu(), y.cpu().unsqueeze(-1)))*y.shape[0]\n",
        "    return sum_loss/total, correct/total, sum_rmse/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOy3gTt_aWEc"
      },
      "source": [
        "#Loading the 50-dimension GloVe Word Embedding file\n",
        "def load_glove_vectors(glove_file):\n",
        "    \"\"\"Load the glove word vectors\"\"\"\n",
        "    word_vectors = {}\n",
        "    with open(glove_file) as f:\n",
        "        for line in f:\n",
        "            split = line.split()\n",
        "            word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n",
        "    return word_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0CDhwi2aWEc"
      },
      "source": [
        "#Creating a GloVe Word Embedding Matrix utilizing the vocab2index variable \n",
        "def get_emb_matrix(pretrained, word_counts,emb_size):\n",
        "    \"\"\" Creates embedding matrix from word vectors\"\"\"\n",
        "    vocab_size = len(word_counts) + 2\n",
        "    vocab_to_idx = {}\n",
        "    vocab = [\"\", \"UNK\"]\n",
        "    W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
        "    W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n",
        "    W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n",
        "    vocab_to_idx[\"UNK\"] = 1\n",
        "    i = 2\n",
        "    for word in word_counts:\n",
        "        if word in word_vecs:\n",
        "            W[i] = word_vecs[word]\n",
        "        else:\n",
        "            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
        "        vocab_to_idx[word] = i\n",
        "        vocab.append(word)\n",
        "        i += 1   \n",
        "    return W, np.array(vocab), vocab_to_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp8_oI_A91pO"
      },
      "source": [
        "# ***SELECT EMBEDDING DIMENSION. GloVe Dimension must match embed_size variable parameter***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXZTVOBI9-GD"
      },
      "source": [
        "# embed options: 50, 100, 200, 300\n",
        "path=\"/content/drive/MyDrive/GloVe/glove.6B.300d.txt\"\n",
        "embed_size=300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8xt9HGKaWEd"
      },
      "source": [
        "#GloVe Embedding Process\n",
        "word_vecs = load_glove_vectors(path)\n",
        "pretrained_weights, vocab, vocab2index = get_emb_matrix(word_vecs, counts,embed_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3uNkRr0aWEd"
      },
      "source": [
        "class LSTM_glove_vecs(torch.nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim,num_layers, hidden_dim, dropout, glove_weights,bidirectional) :\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        #copying GloVe word embedding to model\n",
        "        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights).to(device)) \n",
        "        self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
        "        #LSTM module\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "        #Linear module for the final prediction \n",
        "        #of which emotion class is the processed text\n",
        "        self.linear = nn.Linear(hidden_dim, 4)\n",
        "        #Dropout Regularization for LSTM\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, l):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        lstm_out, (ht, ct) = self.lstm(x)\n",
        "        return self.linear(ht[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE8BBQCTzSNj"
      },
      "source": [
        "# ***Experiment***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "614P0cTUaWEe",
        "outputId": "ab136b97-9dbc-448c-813a-810125f93799"
      },
      "source": [
        "#lists for collecting the loss, accuracy and y_pred\n",
        "train_loss=list()\n",
        "validation_loss=list()\n",
        "accuracy_val=list()\n",
        "prediction=list()\n",
        "\n",
        "#h-params here\n",
        "embedding=300\n",
        "hidden_layers=300\n",
        "num_layers=3\n",
        "epoch=1000\n",
        "dropout=0.1\n",
        "bidirectional=True\n",
        "\n",
        "#Loading the model with the GloVe Word Embedding and given parameters\n",
        "model = LSTM_glove_vecs(vocab_size, \n",
        "                        embedding, \n",
        "                        num_layers, \n",
        "                        hidden_layers,\n",
        "                        dropout, \n",
        "                        pretrained_weights,\n",
        "                        bidirectional)\n",
        "#Utilizing the GPU for the model\n",
        "model.to(device)\n",
        "#training the model \n",
        "stop_epoch=train_model(model, epochs=epoch, lr=0.001, saving_path='/content/drive/MyDrive/GloVe/raw_300d_lstm_d1.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: train loss 1.386, val loss 1.386, train accuracy 0.248, val accuracy 0.242, and val rmse 1.801\n",
            "Epoch 2: train loss 1.385, val loss 1.385, train accuracy 0.248, val accuracy 0.242, and val rmse 1.801\n",
            "Epoch 3: train loss 1.384, val loss 1.384, train accuracy 0.248, val accuracy 0.244, and val rmse 1.795\n",
            "Epoch 4: train loss 1.384, val loss 1.383, train accuracy 0.255, val accuracy 0.261, and val rmse 1.676\n",
            "Epoch 5: train loss 1.383, val loss 1.383, train accuracy 0.279, val accuracy 0.280, and val rmse 1.367\n",
            "Epoch 6: train loss 1.383, val loss 1.383, train accuracy 0.277, val accuracy 0.270, and val rmse 1.225\n",
            "Epoch 7: train loss 1.382, val loss 1.382, train accuracy 0.273, val accuracy 0.264, and val rmse 1.213\n",
            "Epoch 8: train loss 1.382, val loss 1.382, train accuracy 0.268, val accuracy 0.272, and val rmse 1.201\n",
            "Epoch 9: train loss 1.382, val loss 1.382, train accuracy 0.273, val accuracy 0.273, and val rmse 1.207\n",
            "Epoch 10: train loss 1.382, val loss 1.381, train accuracy 0.278, val accuracy 0.267, and val rmse 1.205\n",
            "Epoch 11: train loss 1.381, val loss 1.381, train accuracy 0.275, val accuracy 0.264, and val rmse 1.197\n",
            "Epoch 12: train loss 1.381, val loss 1.381, train accuracy 0.274, val accuracy 0.267, and val rmse 1.201\n",
            "Epoch 13: train loss 1.381, val loss 1.381, train accuracy 0.274, val accuracy 0.269, and val rmse 1.197\n",
            "Epoch 14: train loss 1.381, val loss 1.381, train accuracy 0.279, val accuracy 0.271, and val rmse 1.193\n",
            "Epoch 15: train loss 1.381, val loss 1.381, train accuracy 0.275, val accuracy 0.272, and val rmse 1.190\n",
            "Epoch 16: train loss 1.381, val loss 1.381, train accuracy 0.277, val accuracy 0.274, and val rmse 1.191\n",
            "Epoch 17: train loss 1.381, val loss 1.381, train accuracy 0.279, val accuracy 0.273, and val rmse 1.193\n",
            "Epoch 18: train loss 1.381, val loss 1.381, train accuracy 0.278, val accuracy 0.273, and val rmse 1.191\n",
            "Epoch 19: train loss 1.381, val loss 1.380, train accuracy 0.275, val accuracy 0.275, and val rmse 1.186\n",
            "Epoch 20: train loss 1.381, val loss 1.380, train accuracy 0.279, val accuracy 0.275, and val rmse 1.188\n",
            "Epoch 21: train loss 1.381, val loss 1.380, train accuracy 0.277, val accuracy 0.272, and val rmse 1.189\n",
            "Epoch 22: train loss 1.381, val loss 1.380, train accuracy 0.278, val accuracy 0.273, and val rmse 1.186\n",
            "Epoch 23: train loss 1.381, val loss 1.380, train accuracy 0.282, val accuracy 0.273, and val rmse 1.196\n",
            "Epoch 24: train loss 1.381, val loss 1.380, train accuracy 0.281, val accuracy 0.277, and val rmse 1.181\n",
            "Epoch 25: train loss 1.380, val loss 1.380, train accuracy 0.281, val accuracy 0.275, and val rmse 1.187\n",
            "Epoch 26: train loss 1.380, val loss 1.380, train accuracy 0.278, val accuracy 0.277, and val rmse 1.184\n",
            "Epoch 27: train loss 1.380, val loss 1.380, train accuracy 0.281, val accuracy 0.276, and val rmse 1.183\n",
            "Epoch 28: train loss 1.380, val loss 1.380, train accuracy 0.284, val accuracy 0.278, and val rmse 1.185\n",
            "Epoch 29: train loss 1.380, val loss 1.380, train accuracy 0.282, val accuracy 0.275, and val rmse 1.180\n",
            "Epoch 30: train loss 1.380, val loss 1.380, train accuracy 0.280, val accuracy 0.274, and val rmse 1.190\n",
            "Epoch 31: train loss 1.380, val loss 1.380, train accuracy 0.283, val accuracy 0.277, and val rmse 1.178\n",
            "Epoch 32: train loss 1.380, val loss 1.380, train accuracy 0.284, val accuracy 0.276, and val rmse 1.181\n",
            "Epoch 33: train loss 1.380, val loss 1.380, train accuracy 0.284, val accuracy 0.279, and val rmse 1.180\n",
            "Epoch 34: train loss 1.380, val loss 1.380, train accuracy 0.286, val accuracy 0.277, and val rmse 1.186\n",
            "Epoch 35: train loss 1.380, val loss 1.380, train accuracy 0.283, val accuracy 0.278, and val rmse 1.182\n",
            "Epoch 36: train loss 1.380, val loss 1.380, train accuracy 0.285, val accuracy 0.272, and val rmse 1.187\n",
            "Epoch 37: train loss 1.380, val loss 1.380, train accuracy 0.286, val accuracy 0.274, and val rmse 1.182\n",
            "Epoch 38: train loss 1.380, val loss 1.380, train accuracy 0.287, val accuracy 0.281, and val rmse 1.180\n",
            "Epoch 39: train loss 1.380, val loss 1.380, train accuracy 0.284, val accuracy 0.280, and val rmse 1.178\n",
            "Epoch 40: train loss 1.380, val loss 1.380, train accuracy 0.283, val accuracy 0.277, and val rmse 1.178\n",
            "Epoch 41: train loss 1.380, val loss 1.380, train accuracy 0.283, val accuracy 0.283, and val rmse 1.180\n",
            "Epoch 42: train loss 1.380, val loss 1.380, train accuracy 0.285, val accuracy 0.281, and val rmse 1.177\n",
            "Epoch 43: train loss 1.380, val loss 1.380, train accuracy 0.286, val accuracy 0.279, and val rmse 1.181\n",
            "Epoch 44: train loss 1.380, val loss 1.380, train accuracy 0.288, val accuracy 0.282, and val rmse 1.177\n",
            "Epoch 45: train loss 1.380, val loss 1.380, train accuracy 0.286, val accuracy 0.279, and val rmse 1.183\n",
            "Epoch 46: train loss 1.380, val loss 1.380, train accuracy 0.289, val accuracy 0.282, and val rmse 1.181\n",
            "Epoch 47: train loss 1.380, val loss 1.380, train accuracy 0.286, val accuracy 0.278, and val rmse 1.185\n",
            "Epoch 48: train loss 1.380, val loss 1.380, train accuracy 0.287, val accuracy 0.282, and val rmse 1.185\n",
            "Epoch 49: train loss 1.380, val loss 1.380, train accuracy 0.284, val accuracy 0.285, and val rmse 1.179\n",
            "Epoch 50: train loss 1.380, val loss 1.380, train accuracy 0.289, val accuracy 0.280, and val rmse 1.183\n",
            "Epoch 51: train loss 1.380, val loss 1.380, train accuracy 0.290, val accuracy 0.284, and val rmse 1.179\n",
            "Epoch 52: train loss 1.380, val loss 1.380, train accuracy 0.287, val accuracy 0.282, and val rmse 1.184\n",
            "Epoch 53: train loss 1.380, val loss 1.380, train accuracy 0.289, val accuracy 0.289, and val rmse 1.178\n",
            "Epoch 54: train loss 1.380, val loss 1.380, train accuracy 0.288, val accuracy 0.284, and val rmse 1.182\n",
            "Epoch 55: train loss 1.380, val loss 1.380, train accuracy 0.290, val accuracy 0.282, and val rmse 1.179\n",
            "Epoch 56: train loss 1.380, val loss 1.380, train accuracy 0.292, val accuracy 0.282, and val rmse 1.182\n",
            "Epoch 57: train loss 1.380, val loss 1.380, train accuracy 0.291, val accuracy 0.285, and val rmse 1.176\n",
            "Epoch 58: train loss 1.380, val loss 1.380, train accuracy 0.291, val accuracy 0.283, and val rmse 1.182\n",
            "Epoch 59: train loss 1.380, val loss 1.380, train accuracy 0.292, val accuracy 0.283, and val rmse 1.185\n",
            "Epoch 60: train loss 1.380, val loss 1.380, train accuracy 0.292, val accuracy 0.281, and val rmse 1.176\n",
            "Epoch 61: train loss 1.380, val loss 1.380, train accuracy 0.292, val accuracy 0.286, and val rmse 1.183\n",
            "Epoch 62: train loss 1.380, val loss 1.380, train accuracy 0.295, val accuracy 0.284, and val rmse 1.176\n",
            "Epoch 63: train loss 1.380, val loss 1.380, train accuracy 0.293, val accuracy 0.285, and val rmse 1.179\n",
            "Epoch 64: train loss 1.380, val loss 1.380, train accuracy 0.294, val accuracy 0.285, and val rmse 1.180\n",
            "Epoch 65: train loss 1.380, val loss 1.379, train accuracy 0.294, val accuracy 0.281, and val rmse 1.179\n",
            "Epoch 66: train loss 1.380, val loss 1.379, train accuracy 0.294, val accuracy 0.287, and val rmse 1.186\n",
            "Epoch 67: train loss 1.380, val loss 1.380, train accuracy 0.295, val accuracy 0.280, and val rmse 1.181\n",
            "Epoch 68: train loss 1.380, val loss 1.380, train accuracy 0.293, val accuracy 0.282, and val rmse 1.183\n",
            "Epoch 69: train loss 1.380, val loss 1.379, train accuracy 0.296, val accuracy 0.289, and val rmse 1.173\n",
            "Epoch 70: train loss 1.380, val loss 1.379, train accuracy 0.296, val accuracy 0.288, and val rmse 1.178\n",
            "Epoch 71: train loss 1.380, val loss 1.379, train accuracy 0.294, val accuracy 0.292, and val rmse 1.176\n",
            "Epoch 72: train loss 1.380, val loss 1.379, train accuracy 0.297, val accuracy 0.292, and val rmse 1.177\n",
            "Epoch 73: train loss 1.380, val loss 1.379, train accuracy 0.295, val accuracy 0.287, and val rmse 1.179\n",
            "Epoch 74: train loss 1.380, val loss 1.379, train accuracy 0.296, val accuracy 0.287, and val rmse 1.183\n",
            "Epoch 75: train loss 1.380, val loss 1.379, train accuracy 0.296, val accuracy 0.288, and val rmse 1.183\n",
            "Epoch 76: train loss 1.380, val loss 1.379, train accuracy 0.295, val accuracy 0.289, and val rmse 1.181\n",
            "Epoch 77: train loss 1.380, val loss 1.379, train accuracy 0.299, val accuracy 0.292, and val rmse 1.181\n",
            "Epoch 78: train loss 1.380, val loss 1.379, train accuracy 0.298, val accuracy 0.289, and val rmse 1.175\n",
            "Epoch 79: train loss 1.380, val loss 1.379, train accuracy 0.300, val accuracy 0.293, and val rmse 1.177\n",
            "Epoch 80: train loss 1.380, val loss 1.379, train accuracy 0.297, val accuracy 0.289, and val rmse 1.175\n",
            "Epoch 81: train loss 1.380, val loss 1.379, train accuracy 0.296, val accuracy 0.290, and val rmse 1.175\n",
            "Epoch 82: train loss 1.380, val loss 1.379, train accuracy 0.299, val accuracy 0.288, and val rmse 1.176\n",
            "Epoch 83: train loss 1.379, val loss 1.379, train accuracy 0.301, val accuracy 0.290, and val rmse 1.179\n",
            "Epoch 84: train loss 1.379, val loss 1.379, train accuracy 0.299, val accuracy 0.297, and val rmse 1.173\n",
            "Epoch 85: train loss 1.379, val loss 1.379, train accuracy 0.298, val accuracy 0.295, and val rmse 1.176\n",
            "Epoch 86: train loss 1.379, val loss 1.379, train accuracy 0.301, val accuracy 0.294, and val rmse 1.173\n",
            "Epoch 87: train loss 1.379, val loss 1.379, train accuracy 0.301, val accuracy 0.293, and val rmse 1.180\n",
            "Epoch 88: train loss 1.379, val loss 1.379, train accuracy 0.299, val accuracy 0.292, and val rmse 1.176\n",
            "Epoch 89: train loss 1.379, val loss 1.379, train accuracy 0.299, val accuracy 0.298, and val rmse 1.178\n",
            "Epoch 90: train loss 1.379, val loss 1.379, train accuracy 0.302, val accuracy 0.291, and val rmse 1.171\n",
            "Epoch 91: train loss 1.379, val loss 1.379, train accuracy 0.302, val accuracy 0.296, and val rmse 1.176\n",
            "Epoch 92: train loss 1.379, val loss 1.379, train accuracy 0.301, val accuracy 0.294, and val rmse 1.174\n",
            "Epoch 93: train loss 1.379, val loss 1.379, train accuracy 0.301, val accuracy 0.298, and val rmse 1.172\n",
            "Epoch 94: train loss 1.379, val loss 1.379, train accuracy 0.301, val accuracy 0.297, and val rmse 1.174\n",
            "Epoch 95: train loss 1.379, val loss 1.379, train accuracy 0.304, val accuracy 0.291, and val rmse 1.178\n",
            "Epoch 96: train loss 1.379, val loss 1.379, train accuracy 0.302, val accuracy 0.297, and val rmse 1.174\n",
            "Epoch 97: train loss 1.379, val loss 1.379, train accuracy 0.304, val accuracy 0.294, and val rmse 1.178\n",
            "Epoch 98: train loss 1.379, val loss 1.379, train accuracy 0.304, val accuracy 0.295, and val rmse 1.175\n",
            "Epoch 99: train loss 1.379, val loss 1.379, train accuracy 0.305, val accuracy 0.298, and val rmse 1.178\n",
            "Epoch 100: train loss 1.379, val loss 1.379, train accuracy 0.306, val accuracy 0.298, and val rmse 1.176\n",
            "Epoch 101: train loss 1.379, val loss 1.379, train accuracy 0.305, val accuracy 0.296, and val rmse 1.174\n",
            "Epoch 102: train loss 1.379, val loss 1.379, train accuracy 0.304, val accuracy 0.300, and val rmse 1.168\n",
            "Epoch 103: train loss 1.379, val loss 1.379, train accuracy 0.305, val accuracy 0.298, and val rmse 1.172\n",
            "Epoch 104: train loss 1.379, val loss 1.379, train accuracy 0.305, val accuracy 0.290, and val rmse 1.180\n",
            "Epoch 105: train loss 1.379, val loss 1.379, train accuracy 0.306, val accuracy 0.299, and val rmse 1.178\n",
            "Epoch 106: train loss 1.379, val loss 1.379, train accuracy 0.306, val accuracy 0.298, and val rmse 1.176\n",
            "Epoch 107: train loss 1.379, val loss 1.379, train accuracy 0.307, val accuracy 0.303, and val rmse 1.173\n",
            "Epoch 108: train loss 1.379, val loss 1.379, train accuracy 0.307, val accuracy 0.298, and val rmse 1.169\n",
            "Epoch 109: train loss 1.379, val loss 1.379, train accuracy 0.305, val accuracy 0.301, and val rmse 1.171\n",
            "Epoch 110: train loss 1.379, val loss 1.379, train accuracy 0.309, val accuracy 0.298, and val rmse 1.178\n",
            "Epoch 111: train loss 1.379, val loss 1.379, train accuracy 0.306, val accuracy 0.299, and val rmse 1.181\n",
            "Epoch 112: train loss 1.379, val loss 1.379, train accuracy 0.309, val accuracy 0.297, and val rmse 1.172\n",
            "Epoch 113: train loss 1.379, val loss 1.379, train accuracy 0.309, val accuracy 0.301, and val rmse 1.176\n",
            "Epoch 114: train loss 1.379, val loss 1.379, train accuracy 0.307, val accuracy 0.300, and val rmse 1.176\n",
            "Epoch 115: train loss 1.379, val loss 1.379, train accuracy 0.306, val accuracy 0.301, and val rmse 1.174\n",
            "Epoch 116: train loss 1.379, val loss 1.379, train accuracy 0.307, val accuracy 0.300, and val rmse 1.171\n",
            "Epoch 117: train loss 1.379, val loss 1.379, train accuracy 0.308, val accuracy 0.305, and val rmse 1.171\n",
            "Epoch 118: train loss 1.379, val loss 1.379, train accuracy 0.310, val accuracy 0.301, and val rmse 1.171\n",
            "Epoch 119: train loss 1.379, val loss 1.379, train accuracy 0.308, val accuracy 0.305, and val rmse 1.170\n",
            "Epoch 120: train loss 1.379, val loss 1.379, train accuracy 0.312, val accuracy 0.306, and val rmse 1.170\n",
            "Epoch 121: train loss 1.379, val loss 1.379, train accuracy 0.309, val accuracy 0.306, and val rmse 1.172\n",
            "Epoch 122: train loss 1.379, val loss 1.378, train accuracy 0.312, val accuracy 0.307, and val rmse 1.173\n",
            "Epoch 123: train loss 1.379, val loss 1.378, train accuracy 0.310, val accuracy 0.302, and val rmse 1.176\n",
            "Epoch 124: train loss 1.379, val loss 1.378, train accuracy 0.312, val accuracy 0.303, and val rmse 1.171\n",
            "Epoch 125: train loss 1.379, val loss 1.378, train accuracy 0.311, val accuracy 0.303, and val rmse 1.171\n",
            "Epoch 126: train loss 1.379, val loss 1.378, train accuracy 0.313, val accuracy 0.304, and val rmse 1.172\n",
            "Epoch 127: train loss 1.379, val loss 1.378, train accuracy 0.313, val accuracy 0.299, and val rmse 1.178\n",
            "Epoch 128: train loss 1.379, val loss 1.378, train accuracy 0.314, val accuracy 0.304, and val rmse 1.173\n",
            "Epoch 129: train loss 1.379, val loss 1.378, train accuracy 0.311, val accuracy 0.308, and val rmse 1.171\n",
            "Epoch 130: train loss 1.379, val loss 1.378, train accuracy 0.311, val accuracy 0.303, and val rmse 1.176\n",
            "Epoch 131: train loss 1.379, val loss 1.378, train accuracy 0.311, val accuracy 0.306, and val rmse 1.173\n",
            "Epoch 132: train loss 1.379, val loss 1.378, train accuracy 0.314, val accuracy 0.308, and val rmse 1.173\n",
            "Epoch 133: train loss 1.378, val loss 1.378, train accuracy 0.313, val accuracy 0.304, and val rmse 1.180\n",
            "Epoch 134: train loss 1.379, val loss 1.378, train accuracy 0.311, val accuracy 0.310, and val rmse 1.172\n",
            "Epoch 135: train loss 1.378, val loss 1.378, train accuracy 0.315, val accuracy 0.306, and val rmse 1.171\n",
            "Epoch 136: train loss 1.378, val loss 1.378, train accuracy 0.315, val accuracy 0.313, and val rmse 1.172\n",
            "Epoch 137: train loss 1.378, val loss 1.378, train accuracy 0.317, val accuracy 0.308, and val rmse 1.171\n",
            "Epoch 138: train loss 1.378, val loss 1.378, train accuracy 0.316, val accuracy 0.311, and val rmse 1.164\n",
            "Epoch 139: train loss 1.378, val loss 1.378, train accuracy 0.314, val accuracy 0.308, and val rmse 1.169\n",
            "Epoch 140: train loss 1.378, val loss 1.378, train accuracy 0.316, val accuracy 0.310, and val rmse 1.171\n",
            "Epoch 141: train loss 1.378, val loss 1.378, train accuracy 0.312, val accuracy 0.310, and val rmse 1.168\n",
            "Epoch 142: train loss 1.378, val loss 1.378, train accuracy 0.317, val accuracy 0.312, and val rmse 1.167\n",
            "Epoch 143: train loss 1.378, val loss 1.378, train accuracy 0.315, val accuracy 0.307, and val rmse 1.172\n",
            "Epoch 144: train loss 1.378, val loss 1.378, train accuracy 0.315, val accuracy 0.307, and val rmse 1.170\n",
            "Epoch 145: train loss 1.378, val loss 1.378, train accuracy 0.316, val accuracy 0.304, and val rmse 1.172\n",
            "Epoch 146: train loss 1.378, val loss 1.378, train accuracy 0.316, val accuracy 0.309, and val rmse 1.173\n",
            "Epoch 147: train loss 1.378, val loss 1.378, train accuracy 0.315, val accuracy 0.307, and val rmse 1.169\n",
            "Epoch 148: train loss 1.378, val loss 1.378, train accuracy 0.317, val accuracy 0.313, and val rmse 1.176\n",
            "Epoch 149: train loss 1.378, val loss 1.378, train accuracy 0.316, val accuracy 0.308, and val rmse 1.172\n",
            "Epoch 150: train loss 1.378, val loss 1.378, train accuracy 0.317, val accuracy 0.311, and val rmse 1.170\n",
            "Epoch 151: train loss 1.378, val loss 1.378, train accuracy 0.319, val accuracy 0.315, and val rmse 1.170\n",
            "Epoch 152: train loss 1.378, val loss 1.378, train accuracy 0.320, val accuracy 0.313, and val rmse 1.169\n",
            "Epoch 153: train loss 1.378, val loss 1.378, train accuracy 0.317, val accuracy 0.307, and val rmse 1.172\n",
            "Epoch 154: train loss 1.378, val loss 1.378, train accuracy 0.318, val accuracy 0.310, and val rmse 1.168\n",
            "Epoch 155: train loss 1.378, val loss 1.378, train accuracy 0.320, val accuracy 0.313, and val rmse 1.174\n",
            "Epoch 156: train loss 1.378, val loss 1.378, train accuracy 0.314, val accuracy 0.314, and val rmse 1.173\n",
            "Epoch 157: train loss 1.378, val loss 1.378, train accuracy 0.320, val accuracy 0.310, and val rmse 1.168\n",
            "Epoch 158: train loss 1.378, val loss 1.378, train accuracy 0.321, val accuracy 0.317, and val rmse 1.166\n",
            "Epoch 159: train loss 1.378, val loss 1.378, train accuracy 0.322, val accuracy 0.317, and val rmse 1.164\n",
            "Epoch 160: train loss 1.378, val loss 1.378, train accuracy 0.320, val accuracy 0.313, and val rmse 1.170\n",
            "Epoch 161: train loss 1.378, val loss 1.378, train accuracy 0.320, val accuracy 0.315, and val rmse 1.166\n",
            "Epoch 162: train loss 1.378, val loss 1.378, train accuracy 0.321, val accuracy 0.316, and val rmse 1.169\n",
            "Epoch 163: train loss 1.378, val loss 1.378, train accuracy 0.322, val accuracy 0.318, and val rmse 1.166\n",
            "Epoch 164: train loss 1.378, val loss 1.377, train accuracy 0.322, val accuracy 0.319, and val rmse 1.165\n",
            "Epoch 165: train loss 1.378, val loss 1.377, train accuracy 0.322, val accuracy 0.316, and val rmse 1.170\n",
            "Epoch 166: train loss 1.378, val loss 1.377, train accuracy 0.322, val accuracy 0.316, and val rmse 1.164\n",
            "Epoch 167: train loss 1.378, val loss 1.377, train accuracy 0.322, val accuracy 0.324, and val rmse 1.170\n",
            "Epoch 168: train loss 1.378, val loss 1.377, train accuracy 0.325, val accuracy 0.319, and val rmse 1.169\n",
            "Epoch 169: train loss 1.378, val loss 1.377, train accuracy 0.323, val accuracy 0.315, and val rmse 1.171\n",
            "Epoch 170: train loss 1.378, val loss 1.377, train accuracy 0.321, val accuracy 0.324, and val rmse 1.168\n",
            "Epoch 171: train loss 1.378, val loss 1.377, train accuracy 0.322, val accuracy 0.317, and val rmse 1.162\n",
            "Epoch 172: train loss 1.378, val loss 1.377, train accuracy 0.322, val accuracy 0.323, and val rmse 1.167\n",
            "Epoch 173: train loss 1.378, val loss 1.377, train accuracy 0.323, val accuracy 0.314, and val rmse 1.169\n",
            "Epoch 174: train loss 1.377, val loss 1.377, train accuracy 0.327, val accuracy 0.319, and val rmse 1.165\n",
            "Epoch 175: train loss 1.378, val loss 1.377, train accuracy 0.323, val accuracy 0.318, and val rmse 1.164\n",
            "Epoch 176: train loss 1.377, val loss 1.377, train accuracy 0.325, val accuracy 0.322, and val rmse 1.170\n",
            "Epoch 177: train loss 1.377, val loss 1.377, train accuracy 0.325, val accuracy 0.322, and val rmse 1.167\n",
            "Epoch 178: train loss 1.377, val loss 1.377, train accuracy 0.324, val accuracy 0.320, and val rmse 1.172\n",
            "Epoch 179: train loss 1.377, val loss 1.377, train accuracy 0.324, val accuracy 0.316, and val rmse 1.172\n",
            "Epoch 180: train loss 1.377, val loss 1.377, train accuracy 0.325, val accuracy 0.321, and val rmse 1.163\n",
            "Epoch 181: train loss 1.377, val loss 1.377, train accuracy 0.327, val accuracy 0.320, and val rmse 1.169\n",
            "Epoch 182: train loss 1.377, val loss 1.377, train accuracy 0.325, val accuracy 0.317, and val rmse 1.168\n",
            "Epoch 183: train loss 1.377, val loss 1.377, train accuracy 0.326, val accuracy 0.324, and val rmse 1.166\n",
            "Epoch 184: train loss 1.377, val loss 1.377, train accuracy 0.326, val accuracy 0.324, and val rmse 1.166\n",
            "Epoch 185: train loss 1.377, val loss 1.377, train accuracy 0.326, val accuracy 0.323, and val rmse 1.169\n",
            "Epoch 186: train loss 1.377, val loss 1.377, train accuracy 0.326, val accuracy 0.317, and val rmse 1.165\n",
            "Epoch 187: train loss 1.377, val loss 1.377, train accuracy 0.328, val accuracy 0.324, and val rmse 1.165\n",
            "Epoch 188: train loss 1.377, val loss 1.377, train accuracy 0.328, val accuracy 0.320, and val rmse 1.166\n",
            "Epoch 189: train loss 1.377, val loss 1.377, train accuracy 0.328, val accuracy 0.323, and val rmse 1.163\n",
            "Epoch 190: train loss 1.377, val loss 1.377, train accuracy 0.328, val accuracy 0.322, and val rmse 1.170\n",
            "Epoch 191: train loss 1.377, val loss 1.377, train accuracy 0.328, val accuracy 0.324, and val rmse 1.171\n",
            "Epoch 192: train loss 1.377, val loss 1.377, train accuracy 0.330, val accuracy 0.323, and val rmse 1.167\n",
            "Epoch 193: train loss 1.377, val loss 1.377, train accuracy 0.330, val accuracy 0.321, and val rmse 1.166\n",
            "Epoch 194: train loss 1.377, val loss 1.377, train accuracy 0.330, val accuracy 0.320, and val rmse 1.170\n",
            "Epoch 195: train loss 1.377, val loss 1.377, train accuracy 0.328, val accuracy 0.328, and val rmse 1.166\n",
            "Epoch 196: train loss 1.377, val loss 1.376, train accuracy 0.330, val accuracy 0.327, and val rmse 1.169\n",
            "Epoch 197: train loss 1.377, val loss 1.377, train accuracy 0.329, val accuracy 0.320, and val rmse 1.171\n",
            "Epoch 198: train loss 1.377, val loss 1.376, train accuracy 0.328, val accuracy 0.328, and val rmse 1.163\n",
            "Epoch 199: train loss 1.377, val loss 1.376, train accuracy 0.333, val accuracy 0.329, and val rmse 1.163\n",
            "Epoch 200: train loss 1.377, val loss 1.376, train accuracy 0.330, val accuracy 0.330, and val rmse 1.174\n",
            "Epoch 201: train loss 1.377, val loss 1.376, train accuracy 0.330, val accuracy 0.320, and val rmse 1.168\n",
            "Epoch 202: train loss 1.377, val loss 1.376, train accuracy 0.330, val accuracy 0.327, and val rmse 1.164\n",
            "Epoch 203: train loss 1.377, val loss 1.376, train accuracy 0.331, val accuracy 0.327, and val rmse 1.170\n",
            "Epoch 204: train loss 1.376, val loss 1.376, train accuracy 0.331, val accuracy 0.323, and val rmse 1.164\n",
            "Epoch 205: train loss 1.376, val loss 1.376, train accuracy 0.335, val accuracy 0.326, and val rmse 1.165\n",
            "Epoch 206: train loss 1.376, val loss 1.376, train accuracy 0.332, val accuracy 0.328, and val rmse 1.165\n",
            "Epoch 207: train loss 1.376, val loss 1.376, train accuracy 0.331, val accuracy 0.329, and val rmse 1.169\n",
            "Epoch 208: train loss 1.376, val loss 1.376, train accuracy 0.334, val accuracy 0.329, and val rmse 1.168\n",
            "Epoch 209: train loss 1.376, val loss 1.376, train accuracy 0.330, val accuracy 0.326, and val rmse 1.167\n",
            "Epoch 210: train loss 1.376, val loss 1.376, train accuracy 0.334, val accuracy 0.330, and val rmse 1.165\n",
            "Epoch 211: train loss 1.376, val loss 1.376, train accuracy 0.335, val accuracy 0.333, and val rmse 1.167\n",
            "Epoch 212: train loss 1.376, val loss 1.376, train accuracy 0.332, val accuracy 0.333, and val rmse 1.164\n",
            "Epoch 213: train loss 1.376, val loss 1.376, train accuracy 0.332, val accuracy 0.332, and val rmse 1.159\n",
            "Epoch 214: train loss 1.376, val loss 1.376, train accuracy 0.336, val accuracy 0.332, and val rmse 1.161\n",
            "Epoch 215: train loss 1.376, val loss 1.376, train accuracy 0.335, val accuracy 0.335, and val rmse 1.164\n",
            "Epoch 216: train loss 1.376, val loss 1.376, train accuracy 0.334, val accuracy 0.330, and val rmse 1.164\n",
            "Epoch 217: train loss 1.376, val loss 1.376, train accuracy 0.334, val accuracy 0.328, and val rmse 1.167\n",
            "Epoch 218: train loss 1.376, val loss 1.376, train accuracy 0.336, val accuracy 0.329, and val rmse 1.168\n",
            "Epoch 219: train loss 1.376, val loss 1.376, train accuracy 0.335, val accuracy 0.331, and val rmse 1.161\n",
            "Epoch 220: train loss 1.376, val loss 1.376, train accuracy 0.338, val accuracy 0.331, and val rmse 1.165\n",
            "Epoch 221: train loss 1.376, val loss 1.375, train accuracy 0.336, val accuracy 0.331, and val rmse 1.166\n",
            "Epoch 222: train loss 1.376, val loss 1.376, train accuracy 0.336, val accuracy 0.326, and val rmse 1.164\n",
            "Epoch 223: train loss 1.376, val loss 1.376, train accuracy 0.337, val accuracy 0.331, and val rmse 1.165\n",
            "Epoch 224: train loss 1.376, val loss 1.375, train accuracy 0.338, val accuracy 0.334, and val rmse 1.161\n",
            "Epoch 225: train loss 1.376, val loss 1.375, train accuracy 0.336, val accuracy 0.332, and val rmse 1.167\n",
            "Epoch 226: train loss 1.376, val loss 1.375, train accuracy 0.336, val accuracy 0.330, and val rmse 1.159\n",
            "Epoch 227: train loss 1.376, val loss 1.375, train accuracy 0.338, val accuracy 0.336, and val rmse 1.163\n",
            "Epoch 228: train loss 1.376, val loss 1.375, train accuracy 0.338, val accuracy 0.327, and val rmse 1.163\n",
            "Epoch 229: train loss 1.375, val loss 1.375, train accuracy 0.335, val accuracy 0.334, and val rmse 1.170\n",
            "Epoch 230: train loss 1.375, val loss 1.375, train accuracy 0.337, val accuracy 0.328, and val rmse 1.168\n",
            "Epoch 231: train loss 1.375, val loss 1.375, train accuracy 0.337, val accuracy 0.330, and val rmse 1.166\n",
            "Epoch 232: train loss 1.375, val loss 1.375, train accuracy 0.339, val accuracy 0.335, and val rmse 1.166\n",
            "Epoch 233: train loss 1.375, val loss 1.375, train accuracy 0.340, val accuracy 0.335, and val rmse 1.163\n",
            "Epoch 234: train loss 1.375, val loss 1.375, train accuracy 0.341, val accuracy 0.336, and val rmse 1.161\n",
            "Epoch 235: train loss 1.375, val loss 1.375, train accuracy 0.340, val accuracy 0.331, and val rmse 1.168\n",
            "Epoch 236: train loss 1.375, val loss 1.375, train accuracy 0.342, val accuracy 0.331, and val rmse 1.167\n",
            "Epoch 237: train loss 1.375, val loss 1.375, train accuracy 0.338, val accuracy 0.334, and val rmse 1.162\n",
            "Epoch 238: train loss 1.375, val loss 1.375, train accuracy 0.341, val accuracy 0.332, and val rmse 1.161\n",
            "Epoch 239: train loss 1.375, val loss 1.375, train accuracy 0.343, val accuracy 0.338, and val rmse 1.167\n",
            "Epoch 240: train loss 1.375, val loss 1.375, train accuracy 0.342, val accuracy 0.336, and val rmse 1.163\n",
            "Epoch 241: train loss 1.375, val loss 1.375, train accuracy 0.341, val accuracy 0.338, and val rmse 1.164\n",
            "Epoch 242: train loss 1.375, val loss 1.375, train accuracy 0.342, val accuracy 0.336, and val rmse 1.163\n",
            "Epoch 243: train loss 1.375, val loss 1.375, train accuracy 0.340, val accuracy 0.335, and val rmse 1.171\n",
            "Epoch 244: train loss 1.375, val loss 1.375, train accuracy 0.342, val accuracy 0.337, and val rmse 1.160\n",
            "Epoch 245: train loss 1.375, val loss 1.374, train accuracy 0.341, val accuracy 0.339, and val rmse 1.159\n",
            "Epoch 246: train loss 1.375, val loss 1.374, train accuracy 0.342, val accuracy 0.334, and val rmse 1.167\n",
            "Epoch 247: train loss 1.375, val loss 1.374, train accuracy 0.344, val accuracy 0.335, and val rmse 1.163\n",
            "Epoch 248: train loss 1.375, val loss 1.374, train accuracy 0.342, val accuracy 0.338, and val rmse 1.157\n",
            "Epoch 249: train loss 1.374, val loss 1.374, train accuracy 0.343, val accuracy 0.337, and val rmse 1.160\n",
            "Epoch 250: train loss 1.374, val loss 1.374, train accuracy 0.343, val accuracy 0.335, and val rmse 1.167\n",
            "Epoch 251: train loss 1.374, val loss 1.374, train accuracy 0.343, val accuracy 0.345, and val rmse 1.158\n",
            "Epoch 252: train loss 1.374, val loss 1.374, train accuracy 0.343, val accuracy 0.335, and val rmse 1.156\n",
            "Epoch 253: train loss 1.374, val loss 1.374, train accuracy 0.345, val accuracy 0.339, and val rmse 1.160\n",
            "Epoch 254: train loss 1.374, val loss 1.374, train accuracy 0.344, val accuracy 0.341, and val rmse 1.158\n",
            "Epoch 255: train loss 1.374, val loss 1.374, train accuracy 0.343, val accuracy 0.340, and val rmse 1.155\n",
            "Epoch 256: train loss 1.374, val loss 1.374, train accuracy 0.345, val accuracy 0.338, and val rmse 1.164\n",
            "Epoch 257: train loss 1.374, val loss 1.374, train accuracy 0.342, val accuracy 0.342, and val rmse 1.162\n",
            "Epoch 258: train loss 1.374, val loss 1.374, train accuracy 0.346, val accuracy 0.341, and val rmse 1.160\n",
            "Epoch 259: train loss 1.374, val loss 1.374, train accuracy 0.345, val accuracy 0.338, and val rmse 1.165\n",
            "Epoch 260: train loss 1.374, val loss 1.374, train accuracy 0.345, val accuracy 0.339, and val rmse 1.160\n",
            "Epoch 261: train loss 1.374, val loss 1.373, train accuracy 0.346, val accuracy 0.344, and val rmse 1.159\n",
            "Epoch 262: train loss 1.374, val loss 1.373, train accuracy 0.346, val accuracy 0.341, and val rmse 1.155\n",
            "Epoch 263: train loss 1.374, val loss 1.373, train accuracy 0.344, val accuracy 0.342, and val rmse 1.161\n",
            "Epoch 264: train loss 1.373, val loss 1.373, train accuracy 0.346, val accuracy 0.340, and val rmse 1.162\n",
            "Epoch 265: train loss 1.373, val loss 1.373, train accuracy 0.347, val accuracy 0.341, and val rmse 1.169\n",
            "Epoch 266: train loss 1.373, val loss 1.373, train accuracy 0.345, val accuracy 0.342, and val rmse 1.162\n",
            "Epoch 267: train loss 1.373, val loss 1.373, train accuracy 0.349, val accuracy 0.340, and val rmse 1.163\n",
            "Epoch 268: train loss 1.373, val loss 1.373, train accuracy 0.346, val accuracy 0.344, and val rmse 1.160\n",
            "Epoch 269: train loss 1.373, val loss 1.373, train accuracy 0.348, val accuracy 0.336, and val rmse 1.164\n",
            "Epoch 270: train loss 1.373, val loss 1.373, train accuracy 0.349, val accuracy 0.343, and val rmse 1.159\n",
            "Epoch 271: train loss 1.373, val loss 1.373, train accuracy 0.346, val accuracy 0.343, and val rmse 1.159\n",
            "Epoch 272: train loss 1.373, val loss 1.373, train accuracy 0.345, val accuracy 0.342, and val rmse 1.166\n",
            "Epoch 273: train loss 1.373, val loss 1.373, train accuracy 0.348, val accuracy 0.348, and val rmse 1.155\n",
            "Epoch 274: train loss 1.373, val loss 1.373, train accuracy 0.348, val accuracy 0.348, and val rmse 1.159\n",
            "Epoch 275: train loss 1.373, val loss 1.372, train accuracy 0.347, val accuracy 0.350, and val rmse 1.164\n",
            "Epoch 276: train loss 1.373, val loss 1.372, train accuracy 0.349, val accuracy 0.345, and val rmse 1.159\n",
            "Epoch 277: train loss 1.373, val loss 1.372, train accuracy 0.347, val accuracy 0.344, and val rmse 1.162\n",
            "Epoch 278: train loss 1.373, val loss 1.372, train accuracy 0.351, val accuracy 0.346, and val rmse 1.156\n",
            "Epoch 279: train loss 1.372, val loss 1.372, train accuracy 0.350, val accuracy 0.348, and val rmse 1.162\n",
            "Epoch 280: train loss 1.372, val loss 1.372, train accuracy 0.349, val accuracy 0.342, and val rmse 1.165\n",
            "Epoch 281: train loss 1.372, val loss 1.372, train accuracy 0.349, val accuracy 0.343, and val rmse 1.159\n",
            "Epoch 282: train loss 1.372, val loss 1.372, train accuracy 0.348, val accuracy 0.345, and val rmse 1.164\n",
            "Epoch 283: train loss 1.372, val loss 1.372, train accuracy 0.350, val accuracy 0.344, and val rmse 1.161\n",
            "Epoch 284: train loss 1.372, val loss 1.372, train accuracy 0.349, val accuracy 0.348, and val rmse 1.157\n",
            "Epoch 285: train loss 1.372, val loss 1.372, train accuracy 0.351, val accuracy 0.346, and val rmse 1.161\n",
            "Epoch 286: train loss 1.372, val loss 1.371, train accuracy 0.351, val accuracy 0.353, and val rmse 1.161\n",
            "Epoch 287: train loss 1.372, val loss 1.371, train accuracy 0.350, val accuracy 0.346, and val rmse 1.160\n",
            "Epoch 288: train loss 1.372, val loss 1.371, train accuracy 0.351, val accuracy 0.353, and val rmse 1.161\n",
            "Epoch 289: train loss 1.372, val loss 1.371, train accuracy 0.352, val accuracy 0.351, and val rmse 1.159\n",
            "Epoch 290: train loss 1.371, val loss 1.371, train accuracy 0.351, val accuracy 0.342, and val rmse 1.160\n",
            "Epoch 291: train loss 1.371, val loss 1.371, train accuracy 0.351, val accuracy 0.345, and val rmse 1.161\n",
            "Epoch 292: train loss 1.371, val loss 1.371, train accuracy 0.353, val accuracy 0.351, and val rmse 1.159\n",
            "Epoch 293: train loss 1.371, val loss 1.371, train accuracy 0.351, val accuracy 0.348, and val rmse 1.164\n",
            "Epoch 294: train loss 1.371, val loss 1.371, train accuracy 0.352, val accuracy 0.349, and val rmse 1.164\n",
            "Epoch 295: train loss 1.371, val loss 1.371, train accuracy 0.356, val accuracy 0.351, and val rmse 1.160\n",
            "Epoch 296: train loss 1.371, val loss 1.371, train accuracy 0.354, val accuracy 0.346, and val rmse 1.163\n",
            "Epoch 297: train loss 1.371, val loss 1.370, train accuracy 0.352, val accuracy 0.354, and val rmse 1.160\n",
            "Epoch 298: train loss 1.371, val loss 1.370, train accuracy 0.354, val accuracy 0.354, and val rmse 1.160\n",
            "Epoch 299: train loss 1.370, val loss 1.370, train accuracy 0.353, val accuracy 0.350, and val rmse 1.156\n",
            "Epoch 300: train loss 1.370, val loss 1.370, train accuracy 0.354, val accuracy 0.348, and val rmse 1.157\n",
            "Epoch 301: train loss 1.370, val loss 1.370, train accuracy 0.355, val accuracy 0.349, and val rmse 1.162\n",
            "Epoch 302: train loss 1.370, val loss 1.370, train accuracy 0.354, val accuracy 0.355, and val rmse 1.160\n",
            "Epoch 303: train loss 1.370, val loss 1.370, train accuracy 0.356, val accuracy 0.355, and val rmse 1.163\n",
            "Epoch 304: train loss 1.370, val loss 1.370, train accuracy 0.357, val accuracy 0.355, and val rmse 1.157\n",
            "Epoch 305: train loss 1.370, val loss 1.369, train accuracy 0.356, val accuracy 0.350, and val rmse 1.166\n",
            "Epoch 306: train loss 1.370, val loss 1.369, train accuracy 0.358, val accuracy 0.355, and val rmse 1.160\n",
            "Epoch 307: train loss 1.369, val loss 1.369, train accuracy 0.358, val accuracy 0.351, and val rmse 1.163\n",
            "Epoch 308: train loss 1.369, val loss 1.369, train accuracy 0.360, val accuracy 0.358, and val rmse 1.164\n",
            "Epoch 309: train loss 1.369, val loss 1.369, train accuracy 0.359, val accuracy 0.356, and val rmse 1.162\n",
            "Epoch 310: train loss 1.369, val loss 1.369, train accuracy 0.357, val accuracy 0.354, and val rmse 1.161\n",
            "Epoch 311: train loss 1.369, val loss 1.369, train accuracy 0.358, val accuracy 0.355, and val rmse 1.172\n",
            "Epoch 312: train loss 1.369, val loss 1.369, train accuracy 0.360, val accuracy 0.353, and val rmse 1.163\n",
            "Epoch 313: train loss 1.369, val loss 1.369, train accuracy 0.359, val accuracy 0.354, and val rmse 1.165\n",
            "Epoch 314: train loss 1.369, val loss 1.368, train accuracy 0.358, val accuracy 0.353, and val rmse 1.168\n",
            "Epoch 315: train loss 1.368, val loss 1.368, train accuracy 0.359, val accuracy 0.362, and val rmse 1.160\n",
            "Epoch 316: train loss 1.368, val loss 1.368, train accuracy 0.362, val accuracy 0.355, and val rmse 1.165\n",
            "Epoch 317: train loss 1.368, val loss 1.368, train accuracy 0.361, val accuracy 0.361, and val rmse 1.159\n",
            "Epoch 318: train loss 1.368, val loss 1.368, train accuracy 0.363, val accuracy 0.358, and val rmse 1.168\n",
            "Epoch 319: train loss 1.368, val loss 1.368, train accuracy 0.363, val accuracy 0.360, and val rmse 1.172\n",
            "Epoch 320: train loss 1.368, val loss 1.367, train accuracy 0.363, val accuracy 0.360, and val rmse 1.168\n",
            "Epoch 321: train loss 1.368, val loss 1.367, train accuracy 0.360, val accuracy 0.364, and val rmse 1.168\n",
            "Epoch 322: train loss 1.367, val loss 1.367, train accuracy 0.369, val accuracy 0.355, and val rmse 1.172\n",
            "Epoch 323: train loss 1.367, val loss 1.367, train accuracy 0.363, val accuracy 0.362, and val rmse 1.164\n",
            "Epoch 324: train loss 1.367, val loss 1.367, train accuracy 0.365, val accuracy 0.364, and val rmse 1.173\n",
            "Epoch 325: train loss 1.367, val loss 1.366, train accuracy 0.368, val accuracy 0.362, and val rmse 1.171\n",
            "Epoch 326: train loss 1.367, val loss 1.367, train accuracy 0.364, val accuracy 0.361, and val rmse 1.175\n",
            "Epoch 327: train loss 1.366, val loss 1.366, train accuracy 0.367, val accuracy 0.368, and val rmse 1.176\n",
            "Epoch 328: train loss 1.366, val loss 1.366, train accuracy 0.368, val accuracy 0.370, and val rmse 1.172\n",
            "Epoch 329: train loss 1.366, val loss 1.366, train accuracy 0.368, val accuracy 0.370, and val rmse 1.176\n",
            "Epoch 330: train loss 1.366, val loss 1.366, train accuracy 0.369, val accuracy 0.367, and val rmse 1.176\n",
            "Epoch 331: train loss 1.366, val loss 1.366, train accuracy 0.368, val accuracy 0.369, and val rmse 1.183\n",
            "Epoch 332: train loss 1.366, val loss 1.365, train accuracy 0.370, val accuracy 0.377, and val rmse 1.175\n",
            "Epoch 333: train loss 1.365, val loss 1.365, train accuracy 0.370, val accuracy 0.370, and val rmse 1.182\n",
            "Epoch 334: train loss 1.365, val loss 1.365, train accuracy 0.371, val accuracy 0.376, and val rmse 1.181\n",
            "Epoch 335: train loss 1.365, val loss 1.365, train accuracy 0.370, val accuracy 0.372, and val rmse 1.191\n",
            "Epoch 336: train loss 1.365, val loss 1.364, train accuracy 0.371, val accuracy 0.379, and val rmse 1.180\n",
            "Epoch 337: train loss 1.365, val loss 1.364, train accuracy 0.377, val accuracy 0.373, and val rmse 1.189\n",
            "Epoch 338: train loss 1.364, val loss 1.364, train accuracy 0.370, val accuracy 0.375, and val rmse 1.192\n",
            "Epoch 339: train loss 1.364, val loss 1.363, train accuracy 0.374, val accuracy 0.375, and val rmse 1.191\n",
            "Epoch 340: train loss 1.364, val loss 1.364, train accuracy 0.373, val accuracy 0.374, and val rmse 1.199\n",
            "Epoch 341: train loss 1.364, val loss 1.364, train accuracy 0.375, val accuracy 0.376, and val rmse 1.196\n",
            "Epoch 342: train loss 1.364, val loss 1.363, train accuracy 0.375, val accuracy 0.379, and val rmse 1.196\n",
            "Epoch 343: train loss 1.363, val loss 1.362, train accuracy 0.377, val accuracy 0.380, and val rmse 1.200\n",
            "Epoch 344: train loss 1.363, val loss 1.363, train accuracy 0.379, val accuracy 0.382, and val rmse 1.203\n",
            "Epoch 345: train loss 1.363, val loss 1.363, train accuracy 0.378, val accuracy 0.381, and val rmse 1.202\n",
            "Epoch 346: train loss 1.362, val loss 1.362, train accuracy 0.376, val accuracy 0.376, and val rmse 1.210\n",
            "Epoch 347: train loss 1.362, val loss 1.362, train accuracy 0.378, val accuracy 0.382, and val rmse 1.200\n",
            "Epoch 348: train loss 1.362, val loss 1.362, train accuracy 0.382, val accuracy 0.383, and val rmse 1.206\n",
            "Epoch 349: train loss 1.362, val loss 1.361, train accuracy 0.381, val accuracy 0.380, and val rmse 1.215\n",
            "Epoch 350: train loss 1.361, val loss 1.361, train accuracy 0.382, val accuracy 0.380, and val rmse 1.224\n",
            "Epoch 351: train loss 1.361, val loss 1.361, train accuracy 0.381, val accuracy 0.386, and val rmse 1.219\n",
            "Epoch 352: train loss 1.361, val loss 1.361, train accuracy 0.383, val accuracy 0.383, and val rmse 1.221\n",
            "Epoch 353: train loss 1.360, val loss 1.360, train accuracy 0.385, val accuracy 0.387, and val rmse 1.217\n",
            "Epoch 354: train loss 1.360, val loss 1.360, train accuracy 0.383, val accuracy 0.387, and val rmse 1.233\n",
            "Epoch 355: train loss 1.360, val loss 1.360, train accuracy 0.385, val accuracy 0.389, and val rmse 1.237\n",
            "Epoch 356: train loss 1.359, val loss 1.359, train accuracy 0.383, val accuracy 0.388, and val rmse 1.236\n",
            "Epoch 357: train loss 1.359, val loss 1.359, train accuracy 0.388, val accuracy 0.391, and val rmse 1.241\n",
            "Epoch 358: train loss 1.359, val loss 1.358, train accuracy 0.385, val accuracy 0.395, and val rmse 1.236\n",
            "Epoch 359: train loss 1.358, val loss 1.358, train accuracy 0.386, val accuracy 0.395, and val rmse 1.239\n",
            "Epoch 360: train loss 1.358, val loss 1.358, train accuracy 0.387, val accuracy 0.388, and val rmse 1.249\n",
            "Epoch 361: train loss 1.358, val loss 1.357, train accuracy 0.388, val accuracy 0.394, and val rmse 1.245\n",
            "Epoch 362: train loss 1.357, val loss 1.357, train accuracy 0.388, val accuracy 0.389, and val rmse 1.262\n",
            "Epoch 363: train loss 1.357, val loss 1.356, train accuracy 0.391, val accuracy 0.394, and val rmse 1.258\n",
            "Epoch 364: train loss 1.356, val loss 1.356, train accuracy 0.390, val accuracy 0.390, and val rmse 1.266\n",
            "Epoch 365: train loss 1.356, val loss 1.355, train accuracy 0.389, val accuracy 0.398, and val rmse 1.261\n",
            "Epoch 366: train loss 1.356, val loss 1.355, train accuracy 0.393, val accuracy 0.396, and val rmse 1.276\n",
            "Epoch 367: train loss 1.355, val loss 1.355, train accuracy 0.393, val accuracy 0.397, and val rmse 1.279\n",
            "Epoch 368: train loss 1.355, val loss 1.354, train accuracy 0.389, val accuracy 0.396, and val rmse 1.283\n",
            "Epoch 369: train loss 1.354, val loss 1.353, train accuracy 0.390, val accuracy 0.395, and val rmse 1.293\n",
            "Epoch 370: train loss 1.354, val loss 1.353, train accuracy 0.395, val accuracy 0.399, and val rmse 1.288\n",
            "Epoch 371: train loss 1.353, val loss 1.352, train accuracy 0.394, val accuracy 0.398, and val rmse 1.299\n",
            "Epoch 372: train loss 1.352, val loss 1.352, train accuracy 0.395, val accuracy 0.392, and val rmse 1.282\n",
            "Epoch 373: train loss 1.352, val loss 1.351, train accuracy 0.395, val accuracy 0.400, and val rmse 1.297\n",
            "Epoch 374: train loss 1.351, val loss 1.351, train accuracy 0.396, val accuracy 0.405, and val rmse 1.302\n",
            "Epoch 375: train loss 1.351, val loss 1.351, train accuracy 0.395, val accuracy 0.401, and val rmse 1.309\n",
            "Epoch 376: train loss 1.350, val loss 1.350, train accuracy 0.395, val accuracy 0.398, and val rmse 1.301\n",
            "Epoch 377: train loss 1.350, val loss 1.350, train accuracy 0.399, val accuracy 0.398, and val rmse 1.317\n",
            "Epoch 378: train loss 1.349, val loss 1.348, train accuracy 0.399, val accuracy 0.406, and val rmse 1.314\n",
            "Epoch 379: train loss 1.348, val loss 1.348, train accuracy 0.403, val accuracy 0.406, and val rmse 1.316\n",
            "Epoch 380: train loss 1.348, val loss 1.347, train accuracy 0.399, val accuracy 0.407, and val rmse 1.317\n",
            "Epoch 381: train loss 1.347, val loss 1.346, train accuracy 0.402, val accuracy 0.411, and val rmse 1.323\n",
            "Epoch 382: train loss 1.346, val loss 1.346, train accuracy 0.401, val accuracy 0.404, and val rmse 1.318\n",
            "Epoch 383: train loss 1.345, val loss 1.345, train accuracy 0.399, val accuracy 0.407, and val rmse 1.325\n",
            "Epoch 384: train loss 1.344, val loss 1.344, train accuracy 0.399, val accuracy 0.407, and val rmse 1.325\n",
            "Epoch 385: train loss 1.344, val loss 1.343, train accuracy 0.401, val accuracy 0.407, and val rmse 1.336\n",
            "Epoch 386: train loss 1.343, val loss 1.343, train accuracy 0.403, val accuracy 0.406, and val rmse 1.338\n",
            "Epoch 387: train loss 1.342, val loss 1.341, train accuracy 0.402, val accuracy 0.413, and val rmse 1.346\n",
            "Epoch 388: train loss 1.341, val loss 1.341, train accuracy 0.409, val accuracy 0.410, and val rmse 1.339\n",
            "Epoch 389: train loss 1.340, val loss 1.339, train accuracy 0.410, val accuracy 0.411, and val rmse 1.349\n",
            "Epoch 390: train loss 1.339, val loss 1.338, train accuracy 0.408, val accuracy 0.411, and val rmse 1.344\n",
            "Epoch 391: train loss 1.338, val loss 1.338, train accuracy 0.409, val accuracy 0.413, and val rmse 1.350\n",
            "Epoch 392: train loss 1.336, val loss 1.336, train accuracy 0.414, val accuracy 0.421, and val rmse 1.340\n",
            "Epoch 393: train loss 1.336, val loss 1.335, train accuracy 0.413, val accuracy 0.415, and val rmse 1.359\n",
            "Epoch 394: train loss 1.334, val loss 1.334, train accuracy 0.412, val accuracy 0.417, and val rmse 1.358\n",
            "Epoch 395: train loss 1.333, val loss 1.333, train accuracy 0.414, val accuracy 0.414, and val rmse 1.360\n",
            "Epoch 396: train loss 1.332, val loss 1.331, train accuracy 0.416, val accuracy 0.422, and val rmse 1.344\n",
            "Epoch 397: train loss 1.330, val loss 1.330, train accuracy 0.421, val accuracy 0.422, and val rmse 1.344\n",
            "Epoch 398: train loss 1.329, val loss 1.327, train accuracy 0.422, val accuracy 0.419, and val rmse 1.365\n",
            "Epoch 399: train loss 1.328, val loss 1.328, train accuracy 0.417, val accuracy 0.420, and val rmse 1.360\n",
            "Epoch 400: train loss 1.326, val loss 1.325, train accuracy 0.424, val accuracy 0.425, and val rmse 1.359\n",
            "Epoch 401: train loss 1.324, val loss 1.324, train accuracy 0.421, val accuracy 0.429, and val rmse 1.357\n",
            "Epoch 402: train loss 1.322, val loss 1.322, train accuracy 0.425, val accuracy 0.429, and val rmse 1.361\n",
            "Epoch 403: train loss 1.320, val loss 1.319, train accuracy 0.430, val accuracy 0.428, and val rmse 1.374\n",
            "Epoch 404: train loss 1.318, val loss 1.317, train accuracy 0.426, val accuracy 0.425, and val rmse 1.379\n",
            "Epoch 405: train loss 1.317, val loss 1.316, train accuracy 0.428, val accuracy 0.427, and val rmse 1.373\n",
            "Epoch 406: train loss 1.315, val loss 1.314, train accuracy 0.426, val accuracy 0.424, and val rmse 1.383\n",
            "Epoch 407: train loss 1.312, val loss 1.312, train accuracy 0.433, val accuracy 0.434, and val rmse 1.361\n",
            "Epoch 408: train loss 1.311, val loss 1.309, train accuracy 0.429, val accuracy 0.433, and val rmse 1.364\n",
            "Epoch 409: train loss 1.309, val loss 1.309, train accuracy 0.430, val accuracy 0.440, and val rmse 1.346\n",
            "Epoch 410: train loss 1.306, val loss 1.304, train accuracy 0.434, val accuracy 0.439, and val rmse 1.365\n",
            "Epoch 411: train loss 1.303, val loss 1.302, train accuracy 0.436, val accuracy 0.442, and val rmse 1.350\n",
            "Epoch 412: train loss 1.301, val loss 1.300, train accuracy 0.435, val accuracy 0.437, and val rmse 1.372\n",
            "Epoch 413: train loss 1.298, val loss 1.298, train accuracy 0.439, val accuracy 0.430, and val rmse 1.368\n",
            "Epoch 414: train loss 1.296, val loss 1.295, train accuracy 0.436, val accuracy 0.436, and val rmse 1.376\n",
            "Epoch 415: train loss 1.293, val loss 1.293, train accuracy 0.444, val accuracy 0.428, and val rmse 1.382\n",
            "Epoch 416: train loss 1.291, val loss 1.290, train accuracy 0.440, val accuracy 0.438, and val rmse 1.373\n",
            "Epoch 417: train loss 1.287, val loss 1.287, train accuracy 0.441, val accuracy 0.443, and val rmse 1.372\n",
            "Epoch 418: train loss 1.284, val loss 1.284, train accuracy 0.442, val accuracy 0.445, and val rmse 1.349\n",
            "Epoch 419: train loss 1.281, val loss 1.280, train accuracy 0.442, val accuracy 0.450, and val rmse 1.372\n",
            "Epoch 420: train loss 1.277, val loss 1.278, train accuracy 0.450, val accuracy 0.439, and val rmse 1.375\n",
            "Epoch 421: train loss 1.275, val loss 1.273, train accuracy 0.447, val accuracy 0.440, and val rmse 1.390\n",
            "Epoch 422: train loss 1.273, val loss 1.272, train accuracy 0.444, val accuracy 0.443, and val rmse 1.371\n",
            "Epoch 423: train loss 1.269, val loss 1.271, train accuracy 0.448, val accuracy 0.450, and val rmse 1.371\n",
            "Epoch 424: train loss 1.268, val loss 1.268, train accuracy 0.448, val accuracy 0.446, and val rmse 1.377\n",
            "Epoch 425: train loss 1.263, val loss 1.264, train accuracy 0.449, val accuracy 0.441, and val rmse 1.368\n",
            "Epoch 426: train loss 1.261, val loss 1.258, train accuracy 0.447, val accuracy 0.462, and val rmse 1.353\n",
            "Epoch 427: train loss 1.258, val loss 1.259, train accuracy 0.456, val accuracy 0.449, and val rmse 1.370\n",
            "Epoch 428: train loss 1.254, val loss 1.255, train accuracy 0.452, val accuracy 0.449, and val rmse 1.371\n",
            "Epoch 429: train loss 1.252, val loss 1.254, train accuracy 0.456, val accuracy 0.447, and val rmse 1.383\n",
            "Epoch 430: train loss 1.249, val loss 1.253, train accuracy 0.456, val accuracy 0.456, and val rmse 1.351\n",
            "Epoch 431: train loss 1.247, val loss 1.250, train accuracy 0.455, val accuracy 0.449, and val rmse 1.376\n",
            "Epoch 432: train loss 1.243, val loss 1.252, train accuracy 0.457, val accuracy 0.455, and val rmse 1.367\n",
            "Epoch 433: train loss 1.243, val loss 1.242, train accuracy 0.456, val accuracy 0.458, and val rmse 1.352\n",
            "Epoch 434: train loss 1.239, val loss 1.245, train accuracy 0.458, val accuracy 0.469, and val rmse 1.330\n",
            "Epoch 435: train loss 1.235, val loss 1.240, train accuracy 0.464, val accuracy 0.452, and val rmse 1.374\n",
            "Epoch 436: train loss 1.234, val loss 1.236, train accuracy 0.464, val accuracy 0.457, and val rmse 1.356\n",
            "Epoch 437: train loss 1.232, val loss 1.239, train accuracy 0.463, val accuracy 0.456, and val rmse 1.370\n",
            "Epoch 438: train loss 1.231, val loss 1.232, train accuracy 0.464, val accuracy 0.462, and val rmse 1.354\n",
            "Epoch 439: train loss 1.227, val loss 1.232, train accuracy 0.465, val accuracy 0.457, and val rmse 1.370\n",
            "Epoch 440: train loss 1.225, val loss 1.231, train accuracy 0.467, val accuracy 0.457, and val rmse 1.351\n",
            "Epoch 441: train loss 1.223, val loss 1.230, train accuracy 0.469, val accuracy 0.471, and val rmse 1.352\n",
            "Epoch 442: train loss 1.223, val loss 1.230, train accuracy 0.466, val accuracy 0.465, and val rmse 1.365\n",
            "Epoch 443: train loss 1.220, val loss 1.224, train accuracy 0.475, val accuracy 0.465, and val rmse 1.350\n",
            "Epoch 444: train loss 1.216, val loss 1.220, train accuracy 0.475, val accuracy 0.473, and val rmse 1.344\n",
            "Epoch 445: train loss 1.215, val loss 1.226, train accuracy 0.476, val accuracy 0.466, and val rmse 1.353\n",
            "Epoch 446: train loss 1.213, val loss 1.219, train accuracy 0.476, val accuracy 0.469, and val rmse 1.355\n",
            "Epoch 447: train loss 1.212, val loss 1.221, train accuracy 0.478, val accuracy 0.469, and val rmse 1.346\n",
            "Epoch 448: train loss 1.210, val loss 1.215, train accuracy 0.473, val accuracy 0.477, and val rmse 1.344\n",
            "Epoch 449: train loss 1.207, val loss 1.214, train accuracy 0.480, val accuracy 0.476, and val rmse 1.336\n",
            "Epoch 450: train loss 1.207, val loss 1.209, train accuracy 0.477, val accuracy 0.483, and val rmse 1.324\n",
            "Epoch 451: train loss 1.206, val loss 1.209, train accuracy 0.484, val accuracy 0.476, and val rmse 1.332\n",
            "Epoch 452: train loss 1.200, val loss 1.208, train accuracy 0.482, val accuracy 0.475, and val rmse 1.345\n",
            "Epoch 453: train loss 1.199, val loss 1.209, train accuracy 0.488, val accuracy 0.475, and val rmse 1.349\n",
            "Epoch 454: train loss 1.196, val loss 1.203, train accuracy 0.486, val accuracy 0.483, and val rmse 1.343\n",
            "Epoch 455: train loss 1.197, val loss 1.200, train accuracy 0.485, val accuracy 0.482, and val rmse 1.349\n",
            "Epoch 456: train loss 1.192, val loss 1.201, train accuracy 0.492, val accuracy 0.488, and val rmse 1.335\n",
            "Epoch 457: train loss 1.191, val loss 1.198, train accuracy 0.493, val accuracy 0.479, and val rmse 1.349\n",
            "Epoch 458: train loss 1.188, val loss 1.196, train accuracy 0.488, val accuracy 0.488, and val rmse 1.335\n",
            "Epoch 459: train loss 1.185, val loss 1.196, train accuracy 0.493, val accuracy 0.488, and val rmse 1.322\n",
            "Epoch 460: train loss 1.182, val loss 1.193, train accuracy 0.497, val accuracy 0.490, and val rmse 1.315\n",
            "Epoch 461: train loss 1.181, val loss 1.190, train accuracy 0.493, val accuracy 0.500, and val rmse 1.325\n",
            "Epoch 462: train loss 1.178, val loss 1.185, train accuracy 0.500, val accuracy 0.498, and val rmse 1.311\n",
            "Epoch 463: train loss 1.177, val loss 1.189, train accuracy 0.496, val accuracy 0.486, and val rmse 1.334\n",
            "Epoch 464: train loss 1.175, val loss 1.189, train accuracy 0.504, val accuracy 0.495, and val rmse 1.312\n",
            "Epoch 465: train loss 1.172, val loss 1.180, train accuracy 0.498, val accuracy 0.496, and val rmse 1.326\n",
            "Epoch 466: train loss 1.170, val loss 1.176, train accuracy 0.501, val accuracy 0.493, and val rmse 1.343\n",
            "Epoch 467: train loss 1.170, val loss 1.176, train accuracy 0.506, val accuracy 0.505, and val rmse 1.322\n",
            "Epoch 468: train loss 1.162, val loss 1.174, train accuracy 0.505, val accuracy 0.505, and val rmse 1.313\n",
            "Epoch 469: train loss 1.162, val loss 1.171, train accuracy 0.507, val accuracy 0.506, and val rmse 1.327\n",
            "Epoch 470: train loss 1.161, val loss 1.172, train accuracy 0.508, val accuracy 0.502, and val rmse 1.307\n",
            "Epoch 471: train loss 1.157, val loss 1.165, train accuracy 0.509, val accuracy 0.508, and val rmse 1.334\n",
            "Epoch 472: train loss 1.156, val loss 1.166, train accuracy 0.508, val accuracy 0.506, and val rmse 1.323\n",
            "Epoch 473: train loss 1.152, val loss 1.161, train accuracy 0.516, val accuracy 0.511, and val rmse 1.312\n",
            "Epoch 474: train loss 1.147, val loss 1.164, train accuracy 0.520, val accuracy 0.506, and val rmse 1.318\n",
            "Epoch 475: train loss 1.147, val loss 1.163, train accuracy 0.515, val accuracy 0.507, and val rmse 1.304\n",
            "Epoch 476: train loss 1.146, val loss 1.155, train accuracy 0.516, val accuracy 0.509, and val rmse 1.321\n",
            "Epoch 477: train loss 1.141, val loss 1.151, train accuracy 0.520, val accuracy 0.519, and val rmse 1.314\n",
            "Epoch 478: train loss 1.141, val loss 1.148, train accuracy 0.520, val accuracy 0.526, and val rmse 1.299\n",
            "Epoch 479: train loss 1.133, val loss 1.142, train accuracy 0.523, val accuracy 0.516, and val rmse 1.307\n",
            "Epoch 480: train loss 1.132, val loss 1.140, train accuracy 0.528, val accuracy 0.529, and val rmse 1.293\n",
            "Epoch 481: train loss 1.126, val loss 1.137, train accuracy 0.532, val accuracy 0.517, and val rmse 1.305\n",
            "Epoch 482: train loss 1.125, val loss 1.132, train accuracy 0.529, val accuracy 0.524, and val rmse 1.312\n",
            "Epoch 483: train loss 1.119, val loss 1.128, train accuracy 0.530, val accuracy 0.532, and val rmse 1.278\n",
            "Epoch 484: train loss 1.118, val loss 1.125, train accuracy 0.533, val accuracy 0.524, and val rmse 1.300\n",
            "Epoch 485: train loss 1.110, val loss 1.122, train accuracy 0.538, val accuracy 0.532, and val rmse 1.285\n",
            "Epoch 486: train loss 1.110, val loss 1.119, train accuracy 0.540, val accuracy 0.527, and val rmse 1.303\n",
            "Epoch 487: train loss 1.107, val loss 1.120, train accuracy 0.540, val accuracy 0.530, and val rmse 1.259\n",
            "Epoch 488: train loss 1.103, val loss 1.112, train accuracy 0.541, val accuracy 0.538, and val rmse 1.285\n",
            "Epoch 489: train loss 1.100, val loss 1.109, train accuracy 0.538, val accuracy 0.542, and val rmse 1.280\n",
            "Epoch 490: train loss 1.095, val loss 1.105, train accuracy 0.546, val accuracy 0.536, and val rmse 1.292\n",
            "Epoch 491: train loss 1.093, val loss 1.108, train accuracy 0.540, val accuracy 0.537, and val rmse 1.276\n",
            "Epoch 492: train loss 1.086, val loss 1.098, train accuracy 0.548, val accuracy 0.547, and val rmse 1.258\n",
            "Epoch 493: train loss 1.085, val loss 1.098, train accuracy 0.552, val accuracy 0.544, and val rmse 1.273\n",
            "Epoch 494: train loss 1.082, val loss 1.089, train accuracy 0.553, val accuracy 0.553, and val rmse 1.247\n",
            "Epoch 495: train loss 1.078, val loss 1.088, train accuracy 0.551, val accuracy 0.552, and val rmse 1.248\n",
            "Epoch 496: train loss 1.074, val loss 1.082, train accuracy 0.552, val accuracy 0.554, and val rmse 1.256\n",
            "Epoch 497: train loss 1.072, val loss 1.080, train accuracy 0.553, val accuracy 0.543, and val rmse 1.281\n",
            "Epoch 498: train loss 1.070, val loss 1.080, train accuracy 0.556, val accuracy 0.558, and val rmse 1.269\n",
            "Epoch 499: train loss 1.062, val loss 1.084, train accuracy 0.561, val accuracy 0.553, and val rmse 1.284\n",
            "Epoch 500: train loss 1.060, val loss 1.072, train accuracy 0.562, val accuracy 0.552, and val rmse 1.252\n",
            "Epoch 501: train loss 1.057, val loss 1.064, train accuracy 0.562, val accuracy 0.558, and val rmse 1.250\n",
            "Epoch 502: train loss 1.055, val loss 1.066, train accuracy 0.565, val accuracy 0.559, and val rmse 1.243\n",
            "Epoch 503: train loss 1.048, val loss 1.062, train accuracy 0.568, val accuracy 0.560, and val rmse 1.243\n",
            "Epoch 504: train loss 1.045, val loss 1.064, train accuracy 0.565, val accuracy 0.564, and val rmse 1.265\n",
            "Epoch 505: train loss 1.043, val loss 1.057, train accuracy 0.568, val accuracy 0.563, and val rmse 1.250\n",
            "Epoch 506: train loss 1.041, val loss 1.056, train accuracy 0.569, val accuracy 0.563, and val rmse 1.263\n",
            "Epoch 507: train loss 1.036, val loss 1.053, train accuracy 0.575, val accuracy 0.567, and val rmse 1.243\n",
            "Epoch 508: train loss 1.033, val loss 1.050, train accuracy 0.571, val accuracy 0.562, and val rmse 1.258\n",
            "Epoch 509: train loss 1.031, val loss 1.057, train accuracy 0.571, val accuracy 0.567, and val rmse 1.243\n",
            "Epoch 510: train loss 1.028, val loss 1.057, train accuracy 0.577, val accuracy 0.563, and val rmse 1.229\n",
            "Epoch 511: train loss 1.028, val loss 1.041, train accuracy 0.574, val accuracy 0.576, and val rmse 1.235\n",
            "Epoch 512: train loss 1.025, val loss 1.036, train accuracy 0.580, val accuracy 0.574, and val rmse 1.254\n",
            "Epoch 513: train loss 1.023, val loss 1.038, train accuracy 0.578, val accuracy 0.578, and val rmse 1.215\n",
            "Epoch 514: train loss 1.017, val loss 1.033, train accuracy 0.581, val accuracy 0.575, and val rmse 1.225\n",
            "Epoch 515: train loss 1.018, val loss 1.034, train accuracy 0.583, val accuracy 0.580, and val rmse 1.233\n",
            "Epoch 516: train loss 1.016, val loss 1.029, train accuracy 0.585, val accuracy 0.577, and val rmse 1.230\n",
            "Epoch 517: train loss 1.015, val loss 1.024, train accuracy 0.584, val accuracy 0.575, and val rmse 1.250\n",
            "Epoch 518: train loss 1.010, val loss 1.017, train accuracy 0.588, val accuracy 0.587, and val rmse 1.214\n",
            "Epoch 519: train loss 1.010, val loss 1.025, train accuracy 0.583, val accuracy 0.582, and val rmse 1.225\n",
            "Epoch 520: train loss 1.004, val loss 1.031, train accuracy 0.589, val accuracy 0.579, and val rmse 1.257\n",
            "Epoch 521: train loss 1.003, val loss 1.034, train accuracy 0.591, val accuracy 0.580, and val rmse 1.223\n",
            "Epoch 522: train loss 1.001, val loss 1.022, train accuracy 0.591, val accuracy 0.581, and val rmse 1.224\n",
            "Epoch 523: train loss 0.999, val loss 1.011, train accuracy 0.595, val accuracy 0.597, and val rmse 1.206\n",
            "Epoch 524: train loss 0.996, val loss 1.014, train accuracy 0.594, val accuracy 0.585, and val rmse 1.211\n",
            "Epoch 525: train loss 0.993, val loss 1.018, train accuracy 0.598, val accuracy 0.589, and val rmse 1.241\n",
            "Epoch 526: train loss 0.993, val loss 1.023, train accuracy 0.597, val accuracy 0.577, and val rmse 1.210\n",
            "Epoch 527: train loss 0.991, val loss 1.013, train accuracy 0.598, val accuracy 0.588, and val rmse 1.216\n",
            "Epoch 528: train loss 0.988, val loss 1.006, train accuracy 0.599, val accuracy 0.592, and val rmse 1.208\n",
            "Epoch 529: train loss 0.985, val loss 1.012, train accuracy 0.601, val accuracy 0.587, and val rmse 1.242\n",
            "Epoch 530: train loss 0.986, val loss 1.006, train accuracy 0.600, val accuracy 0.593, and val rmse 1.206\n",
            "Epoch 531: train loss 0.984, val loss 1.002, train accuracy 0.599, val accuracy 0.592, and val rmse 1.209\n",
            "Epoch 532: train loss 0.983, val loss 0.995, train accuracy 0.596, val accuracy 0.597, and val rmse 1.218\n",
            "Epoch 533: train loss 0.979, val loss 1.012, train accuracy 0.603, val accuracy 0.588, and val rmse 1.242\n",
            "Epoch 534: train loss 0.978, val loss 0.996, train accuracy 0.606, val accuracy 0.597, and val rmse 1.223\n",
            "Epoch 535: train loss 0.974, val loss 0.999, train accuracy 0.607, val accuracy 0.597, and val rmse 1.214\n",
            "Epoch 536: train loss 0.975, val loss 0.997, train accuracy 0.606, val accuracy 0.601, and val rmse 1.179\n",
            "Epoch 537: train loss 0.971, val loss 0.995, train accuracy 0.607, val accuracy 0.598, and val rmse 1.195\n",
            "Epoch 538: train loss 0.971, val loss 0.993, train accuracy 0.605, val accuracy 0.601, and val rmse 1.187\n",
            "Epoch 539: train loss 0.968, val loss 0.999, train accuracy 0.610, val accuracy 0.597, and val rmse 1.187\n",
            "Epoch 540: train loss 0.967, val loss 1.000, train accuracy 0.609, val accuracy 0.592, and val rmse 1.204\n",
            "Epoch 541: train loss 0.965, val loss 0.987, train accuracy 0.608, val accuracy 0.611, and val rmse 1.190\n",
            "Epoch 542: train loss 0.965, val loss 0.985, train accuracy 0.614, val accuracy 0.604, and val rmse 1.187\n",
            "Epoch 543: train loss 0.965, val loss 0.990, train accuracy 0.612, val accuracy 0.600, and val rmse 1.179\n",
            "Epoch 544: train loss 0.962, val loss 0.989, train accuracy 0.614, val accuracy 0.602, and val rmse 1.207\n",
            "Epoch 545: train loss 0.961, val loss 0.992, train accuracy 0.614, val accuracy 0.598, and val rmse 1.190\n",
            "Epoch 546: train loss 0.955, val loss 0.983, train accuracy 0.620, val accuracy 0.605, and val rmse 1.195\n",
            "Epoch 547: train loss 0.959, val loss 0.977, train accuracy 0.616, val accuracy 0.611, and val rmse 1.158\n",
            "Epoch 548: train loss 0.953, val loss 0.981, train accuracy 0.618, val accuracy 0.609, and val rmse 1.198\n",
            "Epoch 549: train loss 0.957, val loss 0.972, train accuracy 0.621, val accuracy 0.614, and val rmse 1.180\n",
            "Epoch 550: train loss 0.956, val loss 0.982, train accuracy 0.614, val accuracy 0.607, and val rmse 1.163\n",
            "Epoch 551: train loss 0.949, val loss 0.980, train accuracy 0.623, val accuracy 0.602, and val rmse 1.213\n",
            "Epoch 552: train loss 0.945, val loss 0.973, train accuracy 0.619, val accuracy 0.608, and val rmse 1.197\n",
            "Epoch 553: train loss 0.945, val loss 0.974, train accuracy 0.622, val accuracy 0.608, and val rmse 1.179\n",
            "Epoch 554: train loss 0.939, val loss 0.975, train accuracy 0.623, val accuracy 0.611, and val rmse 1.198\n",
            "Epoch 555: train loss 0.942, val loss 0.966, train accuracy 0.623, val accuracy 0.613, and val rmse 1.188\n",
            "Epoch 556: train loss 0.942, val loss 0.975, train accuracy 0.625, val accuracy 0.605, and val rmse 1.186\n",
            "Epoch 557: train loss 0.938, val loss 0.970, train accuracy 0.627, val accuracy 0.611, and val rmse 1.171\n",
            "Epoch 558: train loss 0.936, val loss 0.961, train accuracy 0.622, val accuracy 0.616, and val rmse 1.166\n",
            "Epoch 559: train loss 0.934, val loss 0.956, train accuracy 0.629, val accuracy 0.617, and val rmse 1.178\n",
            "Epoch 560: train loss 0.936, val loss 0.959, train accuracy 0.628, val accuracy 0.616, and val rmse 1.170\n",
            "Epoch 561: train loss 0.933, val loss 0.954, train accuracy 0.626, val accuracy 0.623, and val rmse 1.157\n",
            "Epoch 562: train loss 0.929, val loss 0.958, train accuracy 0.630, val accuracy 0.617, and val rmse 1.156\n",
            "Epoch 563: train loss 0.928, val loss 0.955, train accuracy 0.632, val accuracy 0.617, and val rmse 1.175\n",
            "Epoch 564: train loss 0.927, val loss 0.956, train accuracy 0.634, val accuracy 0.627, and val rmse 1.143\n",
            "Epoch 565: train loss 0.925, val loss 0.956, train accuracy 0.630, val accuracy 0.617, and val rmse 1.149\n",
            "Epoch 566: train loss 0.923, val loss 0.948, train accuracy 0.636, val accuracy 0.625, and val rmse 1.152\n",
            "Epoch 567: train loss 0.923, val loss 0.960, train accuracy 0.632, val accuracy 0.619, and val rmse 1.160\n",
            "Epoch 568: train loss 0.923, val loss 0.952, train accuracy 0.631, val accuracy 0.626, and val rmse 1.177\n",
            "Epoch 569: train loss 0.919, val loss 0.949, train accuracy 0.638, val accuracy 0.624, and val rmse 1.157\n",
            "Epoch 570: train loss 0.916, val loss 0.951, train accuracy 0.641, val accuracy 0.623, and val rmse 1.141\n",
            "Epoch 571: train loss 0.918, val loss 0.955, train accuracy 0.637, val accuracy 0.623, and val rmse 1.149\n",
            "Epoch 572: train loss 0.916, val loss 0.945, train accuracy 0.638, val accuracy 0.624, and val rmse 1.153\n",
            "Epoch 573: train loss 0.913, val loss 0.950, train accuracy 0.640, val accuracy 0.624, and val rmse 1.148\n",
            "Epoch 574: train loss 0.906, val loss 0.948, train accuracy 0.640, val accuracy 0.616, and val rmse 1.166\n",
            "Epoch 575: train loss 0.907, val loss 0.944, train accuracy 0.642, val accuracy 0.634, and val rmse 1.145\n",
            "Epoch 576: train loss 0.909, val loss 0.937, train accuracy 0.642, val accuracy 0.629, and val rmse 1.139\n",
            "Epoch 577: train loss 0.907, val loss 0.937, train accuracy 0.643, val accuracy 0.627, and val rmse 1.143\n",
            "Epoch 578: train loss 0.906, val loss 0.939, train accuracy 0.640, val accuracy 0.625, and val rmse 1.152\n",
            "Epoch 579: train loss 0.901, val loss 0.947, train accuracy 0.644, val accuracy 0.625, and val rmse 1.126\n",
            "Epoch 580: train loss 0.902, val loss 0.938, train accuracy 0.645, val accuracy 0.629, and val rmse 1.131\n",
            "Epoch 581: train loss 0.900, val loss 0.924, train accuracy 0.644, val accuracy 0.633, and val rmse 1.141\n",
            "Epoch 582: train loss 0.897, val loss 0.925, train accuracy 0.645, val accuracy 0.639, and val rmse 1.142\n",
            "Epoch 583: train loss 0.896, val loss 0.931, train accuracy 0.648, val accuracy 0.636, and val rmse 1.110\n",
            "Epoch 584: train loss 0.894, val loss 0.940, train accuracy 0.647, val accuracy 0.622, and val rmse 1.133\n",
            "Epoch 585: train loss 0.893, val loss 0.930, train accuracy 0.648, val accuracy 0.635, and val rmse 1.156\n",
            "Epoch 586: train loss 0.894, val loss 0.921, train accuracy 0.648, val accuracy 0.633, and val rmse 1.138\n",
            "Epoch 587: train loss 0.894, val loss 0.942, train accuracy 0.648, val accuracy 0.626, and val rmse 1.165\n",
            "Epoch 588: train loss 0.891, val loss 0.917, train accuracy 0.648, val accuracy 0.637, and val rmse 1.133\n",
            "Epoch 589: train loss 0.889, val loss 0.922, train accuracy 0.647, val accuracy 0.640, and val rmse 1.120\n",
            "Epoch 590: train loss 0.885, val loss 0.914, train accuracy 0.654, val accuracy 0.637, and val rmse 1.141\n",
            "Epoch 591: train loss 0.887, val loss 0.919, train accuracy 0.650, val accuracy 0.641, and val rmse 1.112\n",
            "Epoch 592: train loss 0.889, val loss 0.911, train accuracy 0.649, val accuracy 0.641, and val rmse 1.130\n",
            "Epoch 593: train loss 0.885, val loss 0.918, train accuracy 0.652, val accuracy 0.640, and val rmse 1.134\n",
            "Epoch 594: train loss 0.883, val loss 0.909, train accuracy 0.654, val accuracy 0.644, and val rmse 1.139\n",
            "Epoch 595: train loss 0.877, val loss 0.917, train accuracy 0.656, val accuracy 0.637, and val rmse 1.141\n",
            "Epoch 596: train loss 0.882, val loss 0.915, train accuracy 0.652, val accuracy 0.642, and val rmse 1.128\n",
            "Epoch 597: train loss 0.876, val loss 0.910, train accuracy 0.658, val accuracy 0.644, and val rmse 1.119\n",
            "Epoch 598: train loss 0.875, val loss 0.913, train accuracy 0.654, val accuracy 0.641, and val rmse 1.135\n",
            "Epoch 599: train loss 0.872, val loss 0.907, train accuracy 0.657, val accuracy 0.647, and val rmse 1.124\n",
            "Epoch 600: train loss 0.871, val loss 0.910, train accuracy 0.659, val accuracy 0.644, and val rmse 1.113\n",
            "Epoch 601: train loss 0.869, val loss 0.907, train accuracy 0.660, val accuracy 0.647, and val rmse 1.125\n",
            "Epoch 602: train loss 0.874, val loss 0.898, train accuracy 0.657, val accuracy 0.647, and val rmse 1.113\n",
            "Epoch 603: train loss 0.869, val loss 0.899, train accuracy 0.660, val accuracy 0.648, and val rmse 1.119\n",
            "Epoch 604: train loss 0.866, val loss 0.910, train accuracy 0.663, val accuracy 0.642, and val rmse 1.133\n",
            "Epoch 605: train loss 0.868, val loss 0.910, train accuracy 0.662, val accuracy 0.638, and val rmse 1.127\n",
            "Epoch 606: train loss 0.867, val loss 0.898, train accuracy 0.661, val accuracy 0.650, and val rmse 1.108\n",
            "Epoch 607: train loss 0.866, val loss 0.895, train accuracy 0.657, val accuracy 0.652, and val rmse 1.117\n",
            "Epoch 608: train loss 0.867, val loss 0.893, train accuracy 0.661, val accuracy 0.647, and val rmse 1.125\n",
            "Epoch 609: train loss 0.862, val loss 0.887, train accuracy 0.665, val accuracy 0.656, and val rmse 1.114\n",
            "Epoch 610: train loss 0.863, val loss 0.898, train accuracy 0.661, val accuracy 0.644, and val rmse 1.128\n",
            "Epoch 611: train loss 0.860, val loss 0.890, train accuracy 0.664, val accuracy 0.653, and val rmse 1.127\n",
            "Epoch 612: train loss 0.856, val loss 0.878, train accuracy 0.668, val accuracy 0.660, and val rmse 1.106\n",
            "Epoch 613: train loss 0.862, val loss 0.887, train accuracy 0.662, val accuracy 0.651, and val rmse 1.115\n",
            "Epoch 614: train loss 0.858, val loss 0.896, train accuracy 0.664, val accuracy 0.646, and val rmse 1.111\n",
            "Epoch 615: train loss 0.855, val loss 0.899, train accuracy 0.667, val accuracy 0.654, and val rmse 1.126\n",
            "Epoch 616: train loss 0.852, val loss 0.892, train accuracy 0.668, val accuracy 0.650, and val rmse 1.116\n",
            "Epoch 617: train loss 0.852, val loss 0.883, train accuracy 0.666, val accuracy 0.655, and val rmse 1.116\n",
            "Epoch 618: train loss 0.854, val loss 0.887, train accuracy 0.667, val accuracy 0.652, and val rmse 1.105\n",
            "Epoch 619: train loss 0.852, val loss 0.887, train accuracy 0.669, val accuracy 0.656, and val rmse 1.099\n",
            "Epoch 620: train loss 0.850, val loss 0.886, train accuracy 0.669, val accuracy 0.651, and val rmse 1.107\n",
            "Epoch 621: train loss 0.848, val loss 0.894, train accuracy 0.671, val accuracy 0.646, and val rmse 1.123\n",
            "Epoch 622: train loss 0.852, val loss 0.876, train accuracy 0.668, val accuracy 0.655, and val rmse 1.117\n",
            "Epoch 623: train loss 0.841, val loss 0.885, train accuracy 0.672, val accuracy 0.657, and val rmse 1.094\n",
            "Epoch 624: train loss 0.844, val loss 0.886, train accuracy 0.671, val accuracy 0.653, and val rmse 1.131\n",
            "Epoch 625: train loss 0.843, val loss 0.875, train accuracy 0.669, val accuracy 0.667, and val rmse 1.086\n",
            "Epoch 626: train loss 0.841, val loss 0.870, train accuracy 0.674, val accuracy 0.661, and val rmse 1.097\n",
            "Epoch 627: train loss 0.843, val loss 0.873, train accuracy 0.669, val accuracy 0.660, and val rmse 1.094\n",
            "Epoch 628: train loss 0.845, val loss 0.878, train accuracy 0.673, val accuracy 0.656, and val rmse 1.101\n",
            "Epoch 629: train loss 0.844, val loss 0.875, train accuracy 0.671, val accuracy 0.656, and val rmse 1.097\n",
            "Epoch 630: train loss 0.838, val loss 0.886, train accuracy 0.676, val accuracy 0.653, and val rmse 1.096\n",
            "Epoch 631: train loss 0.844, val loss 0.875, train accuracy 0.674, val accuracy 0.658, and val rmse 1.109\n",
            "Epoch 632: train loss 0.838, val loss 0.878, train accuracy 0.675, val accuracy 0.650, and val rmse 1.110\n",
            "Epoch 633: train loss 0.835, val loss 0.877, train accuracy 0.675, val accuracy 0.657, and val rmse 1.090\n",
            "Epoch 634: train loss 0.837, val loss 0.867, train accuracy 0.674, val accuracy 0.656, and val rmse 1.109\n",
            "Epoch 635: train loss 0.834, val loss 0.871, train accuracy 0.676, val accuracy 0.655, and val rmse 1.105\n",
            "Epoch 636: train loss 0.836, val loss 0.873, train accuracy 0.674, val accuracy 0.660, and val rmse 1.105\n",
            "Epoch 637: train loss 0.833, val loss 0.875, train accuracy 0.677, val accuracy 0.658, and val rmse 1.096\n",
            "Epoch 638: train loss 0.831, val loss 0.860, train accuracy 0.677, val accuracy 0.668, and val rmse 1.087\n",
            "Epoch 639: train loss 0.829, val loss 0.865, train accuracy 0.679, val accuracy 0.664, and val rmse 1.089\n",
            "Epoch 640: train loss 0.832, val loss 0.868, train accuracy 0.679, val accuracy 0.663, and val rmse 1.093\n",
            "Epoch 641: train loss 0.829, val loss 0.859, train accuracy 0.681, val accuracy 0.662, and val rmse 1.098\n",
            "Epoch 642: train loss 0.827, val loss 0.871, train accuracy 0.679, val accuracy 0.666, and val rmse 1.096\n",
            "Epoch 643: train loss 0.827, val loss 0.863, train accuracy 0.679, val accuracy 0.662, and val rmse 1.097\n",
            "Epoch 644: train loss 0.827, val loss 0.870, train accuracy 0.679, val accuracy 0.660, and val rmse 1.094\n",
            "Epoch 645: train loss 0.826, val loss 0.859, train accuracy 0.681, val accuracy 0.664, and val rmse 1.079\n",
            "Epoch 646: train loss 0.824, val loss 0.861, train accuracy 0.682, val accuracy 0.667, and val rmse 1.083\n",
            "Epoch 647: train loss 0.825, val loss 0.870, train accuracy 0.679, val accuracy 0.657, and val rmse 1.092\n",
            "Epoch 648: train loss 0.823, val loss 0.863, train accuracy 0.680, val accuracy 0.665, and val rmse 1.072\n",
            "Epoch 649: train loss 0.824, val loss 0.861, train accuracy 0.678, val accuracy 0.666, and val rmse 1.082\n",
            "Epoch 650: train loss 0.820, val loss 0.870, train accuracy 0.683, val accuracy 0.666, and val rmse 1.089\n",
            "Epoch 651: train loss 0.817, val loss 0.864, train accuracy 0.684, val accuracy 0.664, and val rmse 1.100\n",
            "Epoch 652: train loss 0.820, val loss 0.868, train accuracy 0.683, val accuracy 0.667, and val rmse 1.065\n",
            "Epoch 653: train loss 0.821, val loss 0.844, train accuracy 0.681, val accuracy 0.675, and val rmse 1.089\n",
            "Epoch 654: train loss 0.819, val loss 0.849, train accuracy 0.681, val accuracy 0.672, and val rmse 1.083\n",
            "Epoch 655: train loss 0.811, val loss 0.862, train accuracy 0.685, val accuracy 0.663, and val rmse 1.086\n",
            "Epoch 656: train loss 0.819, val loss 0.841, train accuracy 0.680, val accuracy 0.675, and val rmse 1.070\n",
            "Epoch 657: train loss 0.816, val loss 0.852, train accuracy 0.686, val accuracy 0.672, and val rmse 1.080\n",
            "Epoch 658: train loss 0.813, val loss 0.850, train accuracy 0.685, val accuracy 0.668, and val rmse 1.076\n",
            "Epoch 659: train loss 0.814, val loss 0.844, train accuracy 0.687, val accuracy 0.676, and val rmse 1.085\n",
            "Epoch 660: train loss 0.813, val loss 0.856, train accuracy 0.685, val accuracy 0.667, and val rmse 1.099\n",
            "Epoch 661: train loss 0.811, val loss 0.851, train accuracy 0.688, val accuracy 0.670, and val rmse 1.062\n",
            "Epoch 662: train loss 0.811, val loss 0.840, train accuracy 0.684, val accuracy 0.670, and val rmse 1.090\n",
            "Epoch 663: train loss 0.807, val loss 0.843, train accuracy 0.688, val accuracy 0.678, and val rmse 1.074\n",
            "Epoch 664: train loss 0.811, val loss 0.836, train accuracy 0.685, val accuracy 0.676, and val rmse 1.080\n",
            "Epoch 665: train loss 0.807, val loss 0.851, train accuracy 0.686, val accuracy 0.662, and val rmse 1.092\n",
            "Epoch 666: train loss 0.806, val loss 0.848, train accuracy 0.691, val accuracy 0.670, and val rmse 1.084\n",
            "Epoch 667: train loss 0.803, val loss 0.840, train accuracy 0.695, val accuracy 0.682, and val rmse 1.060\n",
            "Epoch 668: train loss 0.809, val loss 0.862, train accuracy 0.688, val accuracy 0.669, and val rmse 1.060\n",
            "Epoch 669: train loss 0.802, val loss 0.837, train accuracy 0.690, val accuracy 0.672, and val rmse 1.063\n",
            "Epoch 670: train loss 0.804, val loss 0.825, train accuracy 0.690, val accuracy 0.679, and val rmse 1.084\n",
            "Epoch 671: train loss 0.798, val loss 0.827, train accuracy 0.693, val accuracy 0.681, and val rmse 1.070\n",
            "Epoch 672: train loss 0.800, val loss 0.846, train accuracy 0.691, val accuracy 0.669, and val rmse 1.078\n",
            "Epoch 673: train loss 0.798, val loss 0.829, train accuracy 0.693, val accuracy 0.683, and val rmse 1.068\n",
            "Epoch 674: train loss 0.801, val loss 0.828, train accuracy 0.689, val accuracy 0.680, and val rmse 1.075\n",
            "Epoch 675: train loss 0.802, val loss 0.839, train accuracy 0.691, val accuracy 0.688, and val rmse 1.048\n",
            "Epoch 676: train loss 0.799, val loss 0.828, train accuracy 0.694, val accuracy 0.681, and val rmse 1.082\n",
            "Epoch 677: train loss 0.795, val loss 0.831, train accuracy 0.692, val accuracy 0.684, and val rmse 1.039\n",
            "Epoch 678: train loss 0.801, val loss 0.832, train accuracy 0.689, val accuracy 0.678, and val rmse 1.056\n",
            "Epoch 679: train loss 0.797, val loss 0.831, train accuracy 0.694, val accuracy 0.686, and val rmse 1.061\n",
            "Epoch 680: train loss 0.793, val loss 0.831, train accuracy 0.697, val accuracy 0.678, and val rmse 1.077\n",
            "Epoch 681: train loss 0.792, val loss 0.835, train accuracy 0.697, val accuracy 0.671, and val rmse 1.086\n",
            "Epoch 682: train loss 0.790, val loss 0.838, train accuracy 0.696, val accuracy 0.675, and val rmse 1.064\n",
            "Epoch 683: train loss 0.789, val loss 0.834, train accuracy 0.695, val accuracy 0.679, and val rmse 1.067\n",
            "Epoch 684: train loss 0.789, val loss 0.827, train accuracy 0.700, val accuracy 0.684, and val rmse 1.065\n",
            "Epoch 685: train loss 0.793, val loss 0.836, train accuracy 0.694, val accuracy 0.675, and val rmse 1.057\n",
            "Epoch 686: train loss 0.790, val loss 0.824, train accuracy 0.695, val accuracy 0.691, and val rmse 1.059\n",
            "Epoch 687: train loss 0.790, val loss 0.824, train accuracy 0.698, val accuracy 0.675, and val rmse 1.076\n",
            "Epoch 688: train loss 0.788, val loss 0.842, train accuracy 0.697, val accuracy 0.674, and val rmse 1.090\n",
            "Epoch 689: train loss 0.788, val loss 0.821, train accuracy 0.694, val accuracy 0.683, and val rmse 1.049\n",
            "Epoch 690: train loss 0.789, val loss 0.815, train accuracy 0.698, val accuracy 0.687, and val rmse 1.059\n",
            "Epoch 691: train loss 0.783, val loss 0.830, train accuracy 0.697, val accuracy 0.675, and val rmse 1.078\n",
            "Epoch 692: train loss 0.787, val loss 0.827, train accuracy 0.694, val accuracy 0.679, and val rmse 1.045\n",
            "Epoch 693: train loss 0.784, val loss 0.834, train accuracy 0.698, val accuracy 0.678, and val rmse 1.042\n",
            "Epoch 694: train loss 0.780, val loss 0.826, train accuracy 0.698, val accuracy 0.676, and val rmse 1.083\n",
            "Epoch 695: train loss 0.780, val loss 0.828, train accuracy 0.700, val accuracy 0.684, and val rmse 1.038\n",
            "Epoch 696: train loss 0.780, val loss 0.819, train accuracy 0.699, val accuracy 0.688, and val rmse 1.059\n",
            "Epoch 697: train loss 0.781, val loss 0.810, train accuracy 0.699, val accuracy 0.698, and val rmse 1.029\n",
            "Epoch 698: train loss 0.782, val loss 0.819, train accuracy 0.699, val accuracy 0.692, and val rmse 1.036\n",
            "Epoch 699: train loss 0.778, val loss 0.820, train accuracy 0.700, val accuracy 0.685, and val rmse 1.034\n",
            "Epoch 700: train loss 0.775, val loss 0.826, train accuracy 0.705, val accuracy 0.686, and val rmse 1.029\n",
            "Epoch 701: train loss 0.780, val loss 0.819, train accuracy 0.698, val accuracy 0.692, and val rmse 1.054\n",
            "Epoch 702: train loss 0.779, val loss 0.820, train accuracy 0.700, val accuracy 0.687, and val rmse 1.070\n",
            "Epoch 703: train loss 0.775, val loss 0.825, train accuracy 0.702, val accuracy 0.683, and val rmse 1.055\n",
            "Epoch 704: train loss 0.776, val loss 0.802, train accuracy 0.701, val accuracy 0.687, and val rmse 1.057\n",
            "Epoch 705: train loss 0.773, val loss 0.835, train accuracy 0.704, val accuracy 0.678, and val rmse 1.054\n",
            "Epoch 706: train loss 0.775, val loss 0.820, train accuracy 0.699, val accuracy 0.683, and val rmse 1.054\n",
            "Epoch 707: train loss 0.778, val loss 0.812, train accuracy 0.704, val accuracy 0.685, and val rmse 1.072\n",
            "Epoch 708: train loss 0.776, val loss 0.816, train accuracy 0.704, val accuracy 0.685, and val rmse 1.065\n",
            "Epoch 709: train loss 0.773, val loss 0.815, train accuracy 0.704, val accuracy 0.688, and val rmse 1.055\n",
            "Epoch 710: train loss 0.771, val loss 0.814, train accuracy 0.704, val accuracy 0.690, and val rmse 1.049\n",
            "Epoch 711: train loss 0.766, val loss 0.805, train accuracy 0.705, val accuracy 0.692, and val rmse 1.058\n",
            "Epoch 712: train loss 0.770, val loss 0.818, train accuracy 0.705, val accuracy 0.685, and val rmse 1.051\n",
            "Epoch 713: train loss 0.769, val loss 0.820, train accuracy 0.708, val accuracy 0.680, and val rmse 1.045\n",
            "Epoch 714: train loss 0.766, val loss 0.810, train accuracy 0.705, val accuracy 0.691, and val rmse 1.044\n",
            "Epoch 715: train loss 0.767, val loss 0.807, train accuracy 0.707, val accuracy 0.689, and val rmse 1.047\n",
            "Epoch 716: train loss 0.766, val loss 0.807, train accuracy 0.707, val accuracy 0.690, and val rmse 1.052\n",
            "Epoch 717: train loss 0.771, val loss 0.828, train accuracy 0.706, val accuracy 0.679, and val rmse 1.046\n",
            "Epoch 718: train loss 0.764, val loss 0.806, train accuracy 0.708, val accuracy 0.695, and val rmse 1.051\n",
            "Epoch 719: train loss 0.765, val loss 0.808, train accuracy 0.707, val accuracy 0.695, and val rmse 1.048\n",
            "Epoch 720: train loss 0.765, val loss 0.822, train accuracy 0.709, val accuracy 0.685, and val rmse 1.025\n",
            "Epoch 721: train loss 0.767, val loss 0.809, train accuracy 0.707, val accuracy 0.682, and val rmse 1.060\n",
            "Epoch 722: train loss 0.762, val loss 0.811, train accuracy 0.709, val accuracy 0.687, and val rmse 1.040\n",
            "Epoch 723: train loss 0.764, val loss 0.809, train accuracy 0.707, val accuracy 0.690, and val rmse 1.040\n",
            "Epoch 724: train loss 0.762, val loss 0.799, train accuracy 0.708, val accuracy 0.693, and val rmse 1.050\n",
            "Epoch 725: train loss 0.763, val loss 0.799, train accuracy 0.706, val accuracy 0.687, and val rmse 1.063\n",
            "Epoch 726: train loss 0.763, val loss 0.799, train accuracy 0.710, val accuracy 0.687, and val rmse 1.060\n",
            "Epoch 727: train loss 0.761, val loss 0.809, train accuracy 0.708, val accuracy 0.688, and val rmse 1.037\n",
            "Epoch 728: train loss 0.761, val loss 0.809, train accuracy 0.705, val accuracy 0.690, and val rmse 1.039\n",
            "Epoch 729: train loss 0.757, val loss 0.798, train accuracy 0.711, val accuracy 0.700, and val rmse 1.028\n",
            "Epoch 730: train loss 0.763, val loss 0.818, train accuracy 0.706, val accuracy 0.686, and val rmse 1.042\n",
            "Epoch 731: train loss 0.757, val loss 0.796, train accuracy 0.708, val accuracy 0.699, and val rmse 1.035\n",
            "Epoch 732: train loss 0.760, val loss 0.806, train accuracy 0.709, val accuracy 0.689, and val rmse 1.060\n",
            "Epoch 733: train loss 0.757, val loss 0.804, train accuracy 0.709, val accuracy 0.697, and val rmse 1.033\n",
            "Epoch 734: train loss 0.759, val loss 0.809, train accuracy 0.711, val accuracy 0.692, and val rmse 1.049\n",
            "Epoch 735: train loss 0.755, val loss 0.793, train accuracy 0.711, val accuracy 0.697, and val rmse 1.038\n",
            "Epoch 736: train loss 0.754, val loss 0.806, train accuracy 0.710, val accuracy 0.696, and val rmse 1.036\n",
            "Epoch 737: train loss 0.756, val loss 0.805, train accuracy 0.708, val accuracy 0.692, and val rmse 1.036\n",
            "Epoch 738: train loss 0.746, val loss 0.789, train accuracy 0.714, val accuracy 0.695, and val rmse 1.067\n",
            "Epoch 739: train loss 0.755, val loss 0.795, train accuracy 0.709, val accuracy 0.697, and val rmse 1.019\n",
            "Epoch 740: train loss 0.751, val loss 0.794, train accuracy 0.713, val accuracy 0.695, and val rmse 1.054\n",
            "Epoch 741: train loss 0.751, val loss 0.790, train accuracy 0.710, val accuracy 0.705, and val rmse 1.037\n",
            "Epoch 742: train loss 0.752, val loss 0.793, train accuracy 0.714, val accuracy 0.697, and val rmse 1.043\n",
            "Epoch 743: train loss 0.751, val loss 0.807, train accuracy 0.714, val accuracy 0.693, and val rmse 1.039\n",
            "Epoch 744: train loss 0.753, val loss 0.795, train accuracy 0.713, val accuracy 0.696, and val rmse 1.036\n",
            "Epoch 745: train loss 0.747, val loss 0.783, train accuracy 0.716, val accuracy 0.702, and val rmse 1.037\n",
            "Epoch 746: train loss 0.749, val loss 0.789, train accuracy 0.710, val accuracy 0.697, and val rmse 1.043\n",
            "Epoch 747: train loss 0.749, val loss 0.784, train accuracy 0.711, val accuracy 0.706, and val rmse 1.048\n",
            "Epoch 748: train loss 0.747, val loss 0.797, train accuracy 0.714, val accuracy 0.696, and val rmse 1.033\n",
            "Epoch 749: train loss 0.740, val loss 0.780, train accuracy 0.720, val accuracy 0.703, and val rmse 1.059\n",
            "Epoch 750: train loss 0.746, val loss 0.794, train accuracy 0.711, val accuracy 0.699, and val rmse 1.034\n",
            "Epoch 751: train loss 0.745, val loss 0.790, train accuracy 0.717, val accuracy 0.699, and val rmse 1.045\n",
            "Epoch 752: train loss 0.745, val loss 0.795, train accuracy 0.714, val accuracy 0.693, and val rmse 1.037\n",
            "Epoch 753: train loss 0.747, val loss 0.787, train accuracy 0.713, val accuracy 0.699, and val rmse 1.038\n",
            "Epoch 754: train loss 0.741, val loss 0.776, train accuracy 0.719, val accuracy 0.699, and val rmse 1.045\n",
            "Epoch 755: train loss 0.741, val loss 0.791, train accuracy 0.721, val accuracy 0.699, and val rmse 1.040\n",
            "Epoch 756: train loss 0.741, val loss 0.786, train accuracy 0.714, val accuracy 0.699, and val rmse 1.038\n",
            "Epoch 757: train loss 0.739, val loss 0.797, train accuracy 0.719, val accuracy 0.696, and val rmse 1.024\n",
            "Epoch 758: train loss 0.744, val loss 0.798, train accuracy 0.715, val accuracy 0.693, and val rmse 1.039\n",
            "Epoch 759: train loss 0.741, val loss 0.799, train accuracy 0.719, val accuracy 0.693, and val rmse 1.026\n",
            "Epoch 760: train loss 0.735, val loss 0.774, train accuracy 0.718, val accuracy 0.707, and val rmse 1.036\n",
            "Epoch 761: train loss 0.738, val loss 0.786, train accuracy 0.718, val accuracy 0.699, and val rmse 1.037\n",
            "Epoch 762: train loss 0.739, val loss 0.777, train accuracy 0.717, val accuracy 0.712, and val rmse 1.016\n",
            "Epoch 763: train loss 0.739, val loss 0.785, train accuracy 0.717, val accuracy 0.700, and val rmse 1.026\n",
            "Epoch 764: train loss 0.733, val loss 0.786, train accuracy 0.717, val accuracy 0.697, and val rmse 1.055\n",
            "Epoch 765: train loss 0.738, val loss 0.788, train accuracy 0.716, val accuracy 0.702, and val rmse 1.026\n",
            "Epoch 766: train loss 0.734, val loss 0.777, train accuracy 0.720, val accuracy 0.713, and val rmse 1.023\n",
            "Epoch 767: train loss 0.738, val loss 0.781, train accuracy 0.719, val accuracy 0.705, and val rmse 1.045\n",
            "Epoch 768: train loss 0.735, val loss 0.781, train accuracy 0.716, val accuracy 0.707, and val rmse 1.027\n",
            "Epoch 769: train loss 0.732, val loss 0.772, train accuracy 0.722, val accuracy 0.703, and val rmse 1.041\n",
            "Epoch 770: train loss 0.735, val loss 0.785, train accuracy 0.720, val accuracy 0.705, and val rmse 1.022\n",
            "Epoch 771: train loss 0.734, val loss 0.774, train accuracy 0.720, val accuracy 0.707, and val rmse 1.035\n",
            "Epoch 772: train loss 0.731, val loss 0.765, train accuracy 0.721, val accuracy 0.712, and val rmse 1.008\n",
            "Epoch 773: train loss 0.728, val loss 0.797, train accuracy 0.722, val accuracy 0.703, and val rmse 1.003\n",
            "Epoch 774: train loss 0.737, val loss 0.787, train accuracy 0.719, val accuracy 0.704, and val rmse 1.030\n",
            "Epoch 775: train loss 0.732, val loss 0.770, train accuracy 0.722, val accuracy 0.708, and val rmse 1.015\n",
            "Epoch 776: train loss 0.728, val loss 0.775, train accuracy 0.723, val accuracy 0.713, and val rmse 1.015\n",
            "Epoch 777: train loss 0.725, val loss 0.781, train accuracy 0.723, val accuracy 0.704, and val rmse 1.028\n",
            "Epoch 778: train loss 0.727, val loss 0.770, train accuracy 0.721, val accuracy 0.716, and val rmse 1.002\n",
            "Epoch 779: train loss 0.728, val loss 0.784, train accuracy 0.725, val accuracy 0.707, and val rmse 1.032\n",
            "Epoch 780: train loss 0.728, val loss 0.778, train accuracy 0.721, val accuracy 0.712, and val rmse 1.024\n",
            "Epoch 781: train loss 0.728, val loss 0.790, train accuracy 0.720, val accuracy 0.696, and val rmse 1.037\n",
            "Epoch 782: train loss 0.727, val loss 0.769, train accuracy 0.723, val accuracy 0.709, and val rmse 1.030\n",
            "Epoch 783: train loss 0.723, val loss 0.772, train accuracy 0.728, val accuracy 0.707, and val rmse 1.023\n",
            "Epoch 784: train loss 0.723, val loss 0.773, train accuracy 0.724, val accuracy 0.707, and val rmse 1.046\n",
            "Epoch 785: train loss 0.723, val loss 0.768, train accuracy 0.725, val accuracy 0.710, and val rmse 1.003\n",
            "Epoch 786: train loss 0.723, val loss 0.757, train accuracy 0.726, val accuracy 0.714, and val rmse 1.020\n",
            "Epoch 787: train loss 0.723, val loss 0.766, train accuracy 0.725, val accuracy 0.713, and val rmse 1.019\n",
            "Epoch 788: train loss 0.724, val loss 0.772, train accuracy 0.725, val accuracy 0.710, and val rmse 1.015\n",
            "Epoch 789: train loss 0.721, val loss 0.798, train accuracy 0.727, val accuracy 0.700, and val rmse 1.030\n",
            "Epoch 790: train loss 0.718, val loss 0.766, train accuracy 0.729, val accuracy 0.710, and val rmse 1.025\n",
            "Epoch 791: train loss 0.721, val loss 0.752, train accuracy 0.727, val accuracy 0.721, and val rmse 0.999\n",
            "Epoch 792: train loss 0.721, val loss 0.760, train accuracy 0.724, val accuracy 0.708, and val rmse 1.043\n",
            "Epoch 793: train loss 0.722, val loss 0.759, train accuracy 0.725, val accuracy 0.712, and val rmse 1.026\n",
            "Epoch 794: train loss 0.721, val loss 0.757, train accuracy 0.724, val accuracy 0.714, and val rmse 1.028\n",
            "Epoch 795: train loss 0.718, val loss 0.758, train accuracy 0.729, val accuracy 0.716, and val rmse 1.018\n",
            "Epoch 796: train loss 0.719, val loss 0.765, train accuracy 0.729, val accuracy 0.715, and val rmse 1.005\n",
            "Epoch 797: train loss 0.718, val loss 0.768, train accuracy 0.728, val accuracy 0.716, and val rmse 1.006\n",
            "Epoch 798: train loss 0.717, val loss 0.778, train accuracy 0.729, val accuracy 0.711, and val rmse 1.015\n",
            "Epoch 799: train loss 0.719, val loss 0.796, train accuracy 0.725, val accuracy 0.700, and val rmse 1.004\n",
            "Epoch 800: train loss 0.716, val loss 0.760, train accuracy 0.731, val accuracy 0.712, and val rmse 1.035\n",
            "Epoch 801: train loss 0.718, val loss 0.760, train accuracy 0.729, val accuracy 0.711, and val rmse 1.028\n",
            "Epoch 802: train loss 0.715, val loss 0.763, train accuracy 0.726, val accuracy 0.716, and val rmse 1.010\n",
            "Epoch 803: train loss 0.712, val loss 0.764, train accuracy 0.728, val accuracy 0.718, and val rmse 1.012\n",
            "Epoch 804: train loss 0.714, val loss 0.771, train accuracy 0.730, val accuracy 0.705, and val rmse 1.029\n",
            "Epoch 805: train loss 0.711, val loss 0.759, train accuracy 0.731, val accuracy 0.717, and val rmse 1.010\n",
            "Epoch 806: train loss 0.712, val loss 0.769, train accuracy 0.728, val accuracy 0.717, and val rmse 0.995\n",
            "Epoch 807: train loss 0.713, val loss 0.771, train accuracy 0.728, val accuracy 0.713, and val rmse 1.011\n",
            "Epoch 808: train loss 0.708, val loss 0.790, train accuracy 0.729, val accuracy 0.699, and val rmse 1.012\n",
            "Epoch 809: train loss 0.711, val loss 0.759, train accuracy 0.729, val accuracy 0.716, and val rmse 1.015\n",
            "Epoch 810: train loss 0.712, val loss 0.777, train accuracy 0.728, val accuracy 0.710, and val rmse 1.005\n",
            "Epoch 811: train loss 0.715, val loss 0.758, train accuracy 0.727, val accuracy 0.720, and val rmse 1.003\n",
            "Epoch 812: train loss 0.713, val loss 0.767, train accuracy 0.733, val accuracy 0.705, and val rmse 1.033\n",
            "Epoch 813: train loss 0.707, val loss 0.757, train accuracy 0.730, val accuracy 0.715, and val rmse 1.020\n",
            "Epoch 814: train loss 0.709, val loss 0.761, train accuracy 0.730, val accuracy 0.720, and val rmse 0.997\n",
            "Epoch 815: train loss 0.710, val loss 0.771, train accuracy 0.730, val accuracy 0.713, and val rmse 1.016\n",
            "Epoch 816: train loss 0.708, val loss 0.769, train accuracy 0.735, val accuracy 0.712, and val rmse 1.015\n",
            "Epoch 817: train loss 0.706, val loss 0.757, train accuracy 0.734, val accuracy 0.714, and val rmse 1.023\n",
            "Epoch 818: train loss 0.707, val loss 0.762, train accuracy 0.732, val accuracy 0.719, and val rmse 1.009\n",
            "Epoch 819: train loss 0.708, val loss 0.779, train accuracy 0.732, val accuracy 0.700, and val rmse 1.007\n",
            "Epoch 820: train loss 0.707, val loss 0.747, train accuracy 0.734, val accuracy 0.718, and val rmse 1.025\n",
            "Epoch 821: train loss 0.709, val loss 0.748, train accuracy 0.733, val accuracy 0.720, and val rmse 1.013\n",
            "Epoch 822: train loss 0.704, val loss 0.768, train accuracy 0.733, val accuracy 0.716, and val rmse 1.019\n",
            "Epoch 823: train loss 0.703, val loss 0.752, train accuracy 0.733, val accuracy 0.718, and val rmse 1.021\n",
            "Epoch 824: train loss 0.706, val loss 0.792, train accuracy 0.734, val accuracy 0.701, and val rmse 1.023\n",
            "Epoch 825: train loss 0.703, val loss 0.758, train accuracy 0.735, val accuracy 0.716, and val rmse 1.020\n",
            "Epoch 826: train loss 0.705, val loss 0.758, train accuracy 0.733, val accuracy 0.715, and val rmse 1.013\n",
            "Epoch 827: train loss 0.704, val loss 0.758, train accuracy 0.735, val accuracy 0.712, and val rmse 1.023\n",
            "Epoch 828: train loss 0.703, val loss 0.747, train accuracy 0.734, val accuracy 0.724, and val rmse 1.012\n",
            "Epoch 829: train loss 0.702, val loss 0.744, train accuracy 0.734, val accuracy 0.715, and val rmse 1.022\n",
            "Epoch 830: train loss 0.705, val loss 0.768, train accuracy 0.732, val accuracy 0.714, and val rmse 1.014\n",
            "Epoch 831: train loss 0.702, val loss 0.764, train accuracy 0.733, val accuracy 0.713, and val rmse 1.006\n",
            "Epoch 832: train loss 0.701, val loss 0.765, train accuracy 0.734, val accuracy 0.710, and val rmse 1.011\n",
            "Epoch 833: train loss 0.700, val loss 0.753, train accuracy 0.736, val accuracy 0.716, and val rmse 1.010\n",
            "Epoch 834: train loss 0.695, val loss 0.758, train accuracy 0.737, val accuracy 0.715, and val rmse 1.014\n",
            "Epoch 835: train loss 0.696, val loss 0.767, train accuracy 0.740, val accuracy 0.713, and val rmse 1.016\n",
            "Epoch 836: train loss 0.697, val loss 0.747, train accuracy 0.740, val accuracy 0.721, and val rmse 1.009\n",
            "Epoch 837: train loss 0.695, val loss 0.752, train accuracy 0.738, val accuracy 0.716, and val rmse 1.024\n",
            "Epoch 838: train loss 0.697, val loss 0.749, train accuracy 0.734, val accuracy 0.719, and val rmse 1.002\n",
            "Epoch 839: train loss 0.697, val loss 0.765, train accuracy 0.737, val accuracy 0.712, and val rmse 1.020\n",
            "Epoch 840: train loss 0.696, val loss 0.770, train accuracy 0.736, val accuracy 0.712, and val rmse 0.980\n",
            "Epoch 841: train loss 0.694, val loss 0.756, train accuracy 0.740, val accuracy 0.713, and val rmse 1.002\n",
            "Epoch 842: train loss 0.698, val loss 0.760, train accuracy 0.736, val accuracy 0.712, and val rmse 1.015\n",
            "Epoch 843: train loss 0.694, val loss 0.752, train accuracy 0.739, val accuracy 0.721, and val rmse 1.007\n",
            "Epoch 844: train loss 0.696, val loss 0.745, train accuracy 0.735, val accuracy 0.715, and val rmse 1.033\n",
            "Epoch 845: train loss 0.695, val loss 0.736, train accuracy 0.738, val accuracy 0.726, and val rmse 1.008\n",
            "Epoch 846: train loss 0.694, val loss 0.773, train accuracy 0.737, val accuracy 0.711, and val rmse 1.025\n",
            "Epoch 847: train loss 0.693, val loss 0.754, train accuracy 0.737, val accuracy 0.722, and val rmse 1.008\n",
            "Epoch 848: train loss 0.693, val loss 0.777, train accuracy 0.736, val accuracy 0.707, and val rmse 0.993\n",
            "Epoch 849: train loss 0.694, val loss 0.744, train accuracy 0.736, val accuracy 0.723, and val rmse 1.002\n",
            "Epoch 850: train loss 0.693, val loss 0.745, train accuracy 0.739, val accuracy 0.724, and val rmse 1.001\n",
            "Epoch 851: train loss 0.692, val loss 0.753, train accuracy 0.739, val accuracy 0.713, and val rmse 1.023\n",
            "Epoch 852: train loss 0.690, val loss 0.734, train accuracy 0.740, val accuracy 0.718, and val rmse 1.025\n",
            "Epoch 853: train loss 0.694, val loss 0.742, train accuracy 0.738, val accuracy 0.720, and val rmse 1.013\n",
            "Epoch 854: train loss 0.685, val loss 0.732, train accuracy 0.743, val accuracy 0.727, and val rmse 1.012\n",
            "Epoch 855: train loss 0.688, val loss 0.778, train accuracy 0.742, val accuracy 0.711, and val rmse 0.992\n",
            "Epoch 856: train loss 0.688, val loss 0.738, train accuracy 0.741, val accuracy 0.722, and val rmse 1.017\n",
            "Epoch 857: train loss 0.689, val loss 0.745, train accuracy 0.740, val accuracy 0.719, and val rmse 1.008\n",
            "Epoch 858: train loss 0.686, val loss 0.748, train accuracy 0.741, val accuracy 0.722, and val rmse 1.025\n",
            "Epoch 859: train loss 0.687, val loss 0.744, train accuracy 0.742, val accuracy 0.728, and val rmse 0.990\n",
            "Epoch 860: train loss 0.685, val loss 0.755, train accuracy 0.744, val accuracy 0.720, and val rmse 0.994\n",
            "Epoch 861: train loss 0.685, val loss 0.739, train accuracy 0.742, val accuracy 0.726, and val rmse 0.996\n",
            "Epoch 862: train loss 0.689, val loss 0.741, train accuracy 0.740, val accuracy 0.720, and val rmse 1.014\n",
            "Epoch 863: train loss 0.688, val loss 0.747, train accuracy 0.741, val accuracy 0.720, and val rmse 1.001\n",
            "Epoch 864: train loss 0.683, val loss 0.734, train accuracy 0.743, val accuracy 0.727, and val rmse 0.996\n",
            "Epoch 865: train loss 0.683, val loss 0.757, train accuracy 0.746, val accuracy 0.710, and val rmse 1.023\n",
            "Epoch 866: train loss 0.680, val loss 0.726, train accuracy 0.743, val accuracy 0.730, and val rmse 0.987\n",
            "Epoch 867: train loss 0.680, val loss 0.734, train accuracy 0.744, val accuracy 0.731, and val rmse 0.982\n",
            "Epoch 868: train loss 0.683, val loss 0.740, train accuracy 0.744, val accuracy 0.722, and val rmse 1.006\n",
            "Epoch 869: train loss 0.682, val loss 0.755, train accuracy 0.745, val accuracy 0.717, and val rmse 1.003\n",
            "Epoch 870: train loss 0.684, val loss 0.728, train accuracy 0.743, val accuracy 0.731, and val rmse 1.003\n",
            "Epoch 871: train loss 0.681, val loss 0.726, train accuracy 0.741, val accuracy 0.728, and val rmse 0.999\n",
            "Epoch 872: train loss 0.682, val loss 0.737, train accuracy 0.746, val accuracy 0.728, and val rmse 1.000\n",
            "Epoch 873: train loss 0.684, val loss 0.763, train accuracy 0.743, val accuracy 0.709, and val rmse 1.002\n",
            "Epoch 874: train loss 0.681, val loss 0.739, train accuracy 0.744, val accuracy 0.724, and val rmse 0.994\n",
            "Epoch 875: train loss 0.681, val loss 0.745, train accuracy 0.744, val accuracy 0.725, and val rmse 1.015\n",
            "Epoch 876: train loss 0.678, val loss 0.732, train accuracy 0.744, val accuracy 0.732, and val rmse 0.987\n",
            "Epoch 877: train loss 0.676, val loss 0.735, train accuracy 0.746, val accuracy 0.723, and val rmse 1.002\n",
            "Epoch 878: train loss 0.679, val loss 0.744, train accuracy 0.744, val accuracy 0.723, and val rmse 0.993\n",
            "Epoch 879: train loss 0.675, val loss 0.782, train accuracy 0.745, val accuracy 0.705, and val rmse 1.003\n",
            "Epoch 880: train loss 0.677, val loss 0.753, train accuracy 0.746, val accuracy 0.720, and val rmse 1.013\n",
            "Epoch 881: train loss 0.673, val loss 0.734, train accuracy 0.749, val accuracy 0.723, and val rmse 1.020\n",
            "Epoch 882: train loss 0.680, val loss 0.721, train accuracy 0.746, val accuracy 0.728, and val rmse 1.000\n",
            "Epoch 883: train loss 0.678, val loss 0.736, train accuracy 0.744, val accuracy 0.729, and val rmse 1.002\n",
            "Epoch 884: train loss 0.677, val loss 0.746, train accuracy 0.744, val accuracy 0.725, and val rmse 0.991\n",
            "Epoch 885: train loss 0.675, val loss 0.722, train accuracy 0.748, val accuracy 0.727, and val rmse 1.004\n",
            "Epoch 886: train loss 0.671, val loss 0.771, train accuracy 0.750, val accuracy 0.711, and val rmse 1.001\n",
            "Epoch 887: train loss 0.671, val loss 0.733, train accuracy 0.748, val accuracy 0.729, and val rmse 1.000\n",
            "Epoch 888: train loss 0.674, val loss 0.745, train accuracy 0.750, val accuracy 0.720, and val rmse 1.012\n",
            "Epoch 889: train loss 0.677, val loss 0.742, train accuracy 0.745, val accuracy 0.723, and val rmse 0.992\n",
            "Epoch 890: train loss 0.673, val loss 0.744, train accuracy 0.750, val accuracy 0.718, and val rmse 0.988\n",
            "Epoch 891: train loss 0.670, val loss 0.732, train accuracy 0.749, val accuracy 0.729, and val rmse 0.980\n",
            "Epoch 892: train loss 0.671, val loss 0.756, train accuracy 0.747, val accuracy 0.714, and val rmse 0.997\n",
            "Epoch 893: train loss 0.667, val loss 0.780, train accuracy 0.750, val accuracy 0.699, and val rmse 1.016\n",
            "Epoch 894: train loss 0.669, val loss 0.721, train accuracy 0.751, val accuracy 0.733, and val rmse 0.983\n",
            "Epoch 895: train loss 0.673, val loss 0.723, train accuracy 0.747, val accuracy 0.727, and val rmse 0.984\n",
            "Epoch 896: train loss 0.669, val loss 0.720, train accuracy 0.749, val accuracy 0.728, and val rmse 0.976\n",
            "Epoch 897: train loss 0.669, val loss 0.740, train accuracy 0.749, val accuracy 0.719, and val rmse 1.028\n",
            "Epoch 898: train loss 0.676, val loss 0.720, train accuracy 0.748, val accuracy 0.721, and val rmse 1.007\n",
            "Epoch 899: train loss 0.668, val loss 0.741, train accuracy 0.752, val accuracy 0.723, and val rmse 0.986\n",
            "Epoch 900: train loss 0.667, val loss 0.744, train accuracy 0.753, val accuracy 0.718, and val rmse 1.005\n",
            "Epoch 901: train loss 0.662, val loss 0.725, train accuracy 0.754, val accuracy 0.729, and val rmse 1.005\n",
            "Epoch 902: train loss 0.670, val loss 0.729, train accuracy 0.747, val accuracy 0.724, and val rmse 1.018\n",
            "Epoch 903: train loss 0.669, val loss 0.715, train accuracy 0.746, val accuracy 0.738, and val rmse 0.982\n",
            "Epoch 904: train loss 0.663, val loss 0.721, train accuracy 0.752, val accuracy 0.733, and val rmse 0.999\n",
            "Epoch 905: train loss 0.666, val loss 0.721, train accuracy 0.750, val accuracy 0.732, and val rmse 1.002\n",
            "Epoch 906: train loss 0.666, val loss 0.737, train accuracy 0.756, val accuracy 0.724, and val rmse 1.014\n",
            "Epoch 907: train loss 0.669, val loss 0.724, train accuracy 0.750, val accuracy 0.729, and val rmse 0.982\n",
            "Epoch 908: train loss 0.665, val loss 0.721, train accuracy 0.753, val accuracy 0.729, and val rmse 1.014\n",
            "Epoch 909: train loss 0.663, val loss 0.717, train accuracy 0.750, val accuracy 0.731, and val rmse 0.986\n",
            "Epoch 910: train loss 0.664, val loss 0.717, train accuracy 0.751, val accuracy 0.739, and val rmse 0.968\n",
            "Epoch 911: train loss 0.662, val loss 0.731, train accuracy 0.751, val accuracy 0.729, and val rmse 0.976\n",
            "Epoch 912: train loss 0.662, val loss 0.768, train accuracy 0.752, val accuracy 0.709, and val rmse 1.003\n",
            "Epoch 913: train loss 0.667, val loss 0.727, train accuracy 0.751, val accuracy 0.730, and val rmse 0.973\n",
            "Epoch 914: train loss 0.665, val loss 0.722, train accuracy 0.747, val accuracy 0.735, and val rmse 0.983\n",
            "Epoch 915: train loss 0.657, val loss 0.728, train accuracy 0.756, val accuracy 0.729, and val rmse 0.984\n",
            "Epoch 916: train loss 0.660, val loss 0.720, train accuracy 0.752, val accuracy 0.735, and val rmse 1.002\n",
            "Epoch 917: train loss 0.659, val loss 0.726, train accuracy 0.755, val accuracy 0.730, and val rmse 0.988\n",
            "Epoch 918: train loss 0.659, val loss 0.713, train accuracy 0.757, val accuracy 0.734, and val rmse 0.978\n",
            "Epoch 919: train loss 0.661, val loss 0.729, train accuracy 0.754, val accuracy 0.727, and val rmse 0.997\n",
            "Epoch 920: train loss 0.661, val loss 0.725, train accuracy 0.752, val accuracy 0.725, and val rmse 1.007\n",
            "Epoch 921: train loss 0.662, val loss 0.716, train accuracy 0.753, val accuracy 0.730, and val rmse 0.992\n",
            "Epoch 922: train loss 0.658, val loss 0.730, train accuracy 0.754, val accuracy 0.725, and val rmse 1.004\n",
            "Epoch 923: train loss 0.657, val loss 0.736, train accuracy 0.756, val accuracy 0.721, and val rmse 1.000\n",
            "Epoch 924: train loss 0.662, val loss 0.724, train accuracy 0.751, val accuracy 0.728, and val rmse 0.994\n",
            "Epoch 925: train loss 0.657, val loss 0.711, train accuracy 0.754, val accuracy 0.730, and val rmse 1.004\n",
            "Epoch 926: train loss 0.657, val loss 0.726, train accuracy 0.756, val accuracy 0.731, and val rmse 0.986\n",
            "Epoch 927: train loss 0.656, val loss 0.724, train accuracy 0.751, val accuracy 0.728, and val rmse 1.018\n",
            "Epoch 928: train loss 0.652, val loss 0.731, train accuracy 0.754, val accuracy 0.731, and val rmse 1.018\n",
            "Epoch 929: train loss 0.655, val loss 0.750, train accuracy 0.757, val accuracy 0.724, and val rmse 0.977\n",
            "Epoch 930: train loss 0.659, val loss 0.734, train accuracy 0.751, val accuracy 0.724, and val rmse 0.980\n",
            "Epoch 931: train loss 0.655, val loss 0.715, train accuracy 0.757, val accuracy 0.737, and val rmse 0.990\n",
            "Epoch 932: train loss 0.656, val loss 0.759, train accuracy 0.753, val accuracy 0.713, and val rmse 1.022\n",
            "Epoch 933: train loss 0.655, val loss 0.717, train accuracy 0.756, val accuracy 0.737, and val rmse 0.964\n",
            "Epoch 934: train loss 0.657, val loss 0.716, train accuracy 0.754, val accuracy 0.733, and val rmse 0.995\n",
            "Epoch 935: train loss 0.652, val loss 0.740, train accuracy 0.757, val accuracy 0.731, and val rmse 0.993\n",
            "Epoch 936: train loss 0.650, val loss 0.716, train accuracy 0.756, val accuracy 0.728, and val rmse 0.995\n",
            "Epoch 937: train loss 0.658, val loss 0.735, train accuracy 0.755, val accuracy 0.728, and val rmse 0.981\n",
            "Epoch 938: train loss 0.661, val loss 0.744, train accuracy 0.753, val accuracy 0.725, and val rmse 0.983\n",
            "Epoch 939: train loss 0.653, val loss 0.744, train accuracy 0.757, val accuracy 0.723, and val rmse 0.975\n",
            "Epoch 940: train loss 0.663, val loss 0.710, train accuracy 0.752, val accuracy 0.740, and val rmse 0.973\n",
            "Epoch 941: train loss 0.648, val loss 0.757, train accuracy 0.760, val accuracy 0.715, and val rmse 0.997\n",
            "Epoch 942: train loss 0.655, val loss 0.708, train accuracy 0.757, val accuracy 0.734, and val rmse 0.996\n",
            "Epoch 943: train loss 0.648, val loss 0.724, train accuracy 0.758, val accuracy 0.724, and val rmse 1.014\n",
            "Epoch 944: train loss 0.651, val loss 0.724, train accuracy 0.756, val accuracy 0.731, and val rmse 0.984\n",
            "Epoch 945: train loss 0.650, val loss 0.749, train accuracy 0.758, val accuracy 0.721, and val rmse 0.985\n",
            "Epoch 946: train loss 0.649, val loss 0.710, train accuracy 0.760, val accuracy 0.734, and val rmse 0.987\n",
            "Epoch 947: train loss 0.651, val loss 0.717, train accuracy 0.758, val accuracy 0.728, and val rmse 1.004\n",
            "Epoch 948: train loss 0.654, val loss 0.716, train accuracy 0.754, val accuracy 0.738, and val rmse 0.983\n",
            "Epoch 949: train loss 0.647, val loss 0.707, train accuracy 0.758, val accuracy 0.741, and val rmse 0.973\n",
            "Epoch 950: train loss 0.651, val loss 0.723, train accuracy 0.759, val accuracy 0.731, and val rmse 0.995\n",
            "Epoch 951: train loss 0.650, val loss 0.715, train accuracy 0.756, val accuracy 0.731, and val rmse 1.003\n",
            "Epoch 952: train loss 0.648, val loss 0.708, train accuracy 0.757, val accuracy 0.736, and val rmse 0.991\n",
            "Epoch 953: train loss 0.642, val loss 0.707, train accuracy 0.762, val accuracy 0.735, and val rmse 0.982\n",
            "Epoch 954: train loss 0.647, val loss 0.744, train accuracy 0.761, val accuracy 0.718, and val rmse 0.973\n",
            "Epoch 955: train loss 0.641, val loss 0.710, train accuracy 0.763, val accuracy 0.734, and val rmse 0.980\n",
            "Epoch 956: train loss 0.644, val loss 0.712, train accuracy 0.758, val accuracy 0.731, and val rmse 1.005\n",
            "Epoch 957: train loss 0.643, val loss 0.725, train accuracy 0.757, val accuracy 0.730, and val rmse 0.976\n",
            "Epoch 958: train loss 0.648, val loss 0.738, train accuracy 0.759, val accuracy 0.719, and val rmse 0.960\n",
            "Epoch 959: train loss 0.646, val loss 0.716, train accuracy 0.760, val accuracy 0.738, and val rmse 0.968\n",
            "Epoch 960: train loss 0.641, val loss 0.701, train accuracy 0.762, val accuracy 0.743, and val rmse 0.963\n",
            "Epoch 961: train loss 0.641, val loss 0.716, train accuracy 0.764, val accuracy 0.734, and val rmse 0.987\n",
            "Epoch 962: train loss 0.642, val loss 0.722, train accuracy 0.761, val accuracy 0.724, and val rmse 1.007\n",
            "Epoch 963: train loss 0.644, val loss 0.726, train accuracy 0.761, val accuracy 0.739, and val rmse 0.952\n",
            "Epoch 964: train loss 0.645, val loss 0.723, train accuracy 0.760, val accuracy 0.722, and val rmse 1.009\n",
            "Epoch 965: train loss 0.637, val loss 0.714, train accuracy 0.764, val accuracy 0.736, and val rmse 0.977\n",
            "Epoch 966: train loss 0.643, val loss 0.716, train accuracy 0.761, val accuracy 0.735, and val rmse 0.985\n",
            "Epoch 967: train loss 0.640, val loss 0.711, train accuracy 0.763, val accuracy 0.735, and val rmse 0.982\n",
            "Epoch 968: train loss 0.638, val loss 0.720, train accuracy 0.763, val accuracy 0.733, and val rmse 0.989\n",
            "Epoch 969: train loss 0.643, val loss 0.694, train accuracy 0.761, val accuracy 0.743, and val rmse 0.962\n",
            "Epoch 970: train loss 0.640, val loss 0.772, train accuracy 0.763, val accuracy 0.700, and val rmse 0.991\n",
            "Epoch 971: train loss 0.643, val loss 0.699, train accuracy 0.760, val accuracy 0.744, and val rmse 0.969\n",
            "Epoch 972: train loss 0.643, val loss 0.691, train accuracy 0.762, val accuracy 0.745, and val rmse 0.963\n",
            "Epoch 973: train loss 0.638, val loss 0.705, train accuracy 0.763, val accuracy 0.738, and val rmse 0.985\n",
            "Epoch 974: train loss 0.641, val loss 0.704, train accuracy 0.761, val accuracy 0.744, and val rmse 0.968\n",
            "Epoch 975: train loss 0.638, val loss 0.705, train accuracy 0.762, val accuracy 0.733, and val rmse 0.975\n",
            "Epoch 976: train loss 0.639, val loss 0.700, train accuracy 0.763, val accuracy 0.736, and val rmse 0.980\n",
            "Epoch 977: train loss 0.634, val loss 0.716, train accuracy 0.764, val accuracy 0.736, and val rmse 0.961\n",
            "Epoch 978: train loss 0.639, val loss 0.695, train accuracy 0.764, val accuracy 0.743, and val rmse 0.963\n",
            "Epoch 979: train loss 0.641, val loss 0.703, train accuracy 0.759, val accuracy 0.739, and val rmse 0.972\n",
            "Epoch 980: train loss 0.638, val loss 0.703, train accuracy 0.762, val accuracy 0.743, and val rmse 0.980\n",
            "Epoch 981: train loss 0.636, val loss 0.712, train accuracy 0.764, val accuracy 0.732, and val rmse 0.979\n",
            "Epoch 982: train loss 0.637, val loss 0.709, train accuracy 0.761, val accuracy 0.735, and val rmse 0.983\n",
            "Epoch 983: train loss 0.631, val loss 0.724, train accuracy 0.764, val accuracy 0.724, and val rmse 1.001\n",
            "Epoch 984: train loss 0.637, val loss 0.711, train accuracy 0.761, val accuracy 0.734, and val rmse 0.988\n",
            "Epoch 985: train loss 0.633, val loss 0.697, train accuracy 0.764, val accuracy 0.743, and val rmse 0.971\n",
            "Epoch 986: train loss 0.635, val loss 0.710, train accuracy 0.764, val accuracy 0.734, and val rmse 0.992\n",
            "Epoch 987: train loss 0.633, val loss 0.684, train accuracy 0.761, val accuracy 0.746, and val rmse 0.952\n",
            "Epoch 988: train loss 0.631, val loss 0.692, train accuracy 0.764, val accuracy 0.744, and val rmse 0.962\n",
            "Epoch 989: train loss 0.637, val loss 0.713, train accuracy 0.765, val accuracy 0.735, and val rmse 0.985\n",
            "Epoch 990: train loss 0.639, val loss 0.717, train accuracy 0.765, val accuracy 0.738, and val rmse 0.954\n",
            "Epoch 991: train loss 0.631, val loss 0.695, train accuracy 0.763, val accuracy 0.741, and val rmse 0.979\n",
            "Epoch 992: train loss 0.630, val loss 0.725, train accuracy 0.766, val accuracy 0.737, and val rmse 0.971\n",
            "Epoch 993: train loss 0.629, val loss 0.706, train accuracy 0.768, val accuracy 0.739, and val rmse 0.972\n",
            "Epoch 994: train loss 0.634, val loss 0.714, train accuracy 0.761, val accuracy 0.729, and val rmse 0.994\n",
            "Epoch 995: train loss 0.626, val loss 0.700, train accuracy 0.769, val accuracy 0.740, and val rmse 0.984\n",
            "Epoch 996: train loss 0.632, val loss 0.699, train accuracy 0.766, val accuracy 0.740, and val rmse 0.982\n",
            "Epoch 997: train loss 0.629, val loss 0.701, train accuracy 0.765, val accuracy 0.741, and val rmse 0.975\n",
            "Epoch 998: train loss 0.629, val loss 0.710, train accuracy 0.763, val accuracy 0.738, and val rmse 0.970\n",
            "Epoch 999: train loss 0.640, val loss 0.687, train accuracy 0.761, val accuracy 0.745, and val rmse 0.981\n",
            "Epoch 1000: train loss 0.632, val loss 0.679, train accuracy 0.764, val accuracy 0.750, and val rmse 0.937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PstOo0oC-RT4",
        "outputId": "9eddbdb7-7dbc-4c93-cb9b-2b584477d99c"
      },
      "source": [
        "print('Best model generated is at epoch: {0}'.format(stop_epoch+1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best model generated is at epoch: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta2IxeWJsTgl"
      },
      "source": [
        "# ***Confusion Matrix***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sALS6DNvGvx-"
      },
      "source": [
        "#Getting the y values (label)\n",
        "targets=list()\n",
        "for x,y,l in val_dl:\n",
        "  for z in y:\n",
        "    targets.append(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTOwyHKVGxmy"
      },
      "source": [
        "preds=list()\n",
        "#Getting the last 48 records of the prediction list \n",
        "#as the last 48 records contains the total 20% validation data prediction \n",
        "#which represents the y_pred generated\n",
        "#by the prediction model\n",
        "temp=prediction[(stop_epoch*64):(stop_epoch*64)+64]\n",
        "for i in range (0,len(temp)):\n",
        "  for z in temp[i]:\n",
        "    preds.append(z.cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11apwZxF9xk6",
        "outputId": "8e2b636f-7a78-4f22-a2c0-fb05c81d3d25"
      },
      "source": [
        "#Confusion Matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_true = targets\n",
        "y_pred = preds\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[648  33  34 137]\n",
            " [ 34 793 109 161]\n",
            " [ 22  63 905 124]\n",
            " [ 85  74 133 685]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.76      0.79       852\n",
            "           1       0.82      0.72      0.77      1097\n",
            "           2       0.77      0.81      0.79      1114\n",
            "           3       0.62      0.70      0.66       977\n",
            "\n",
            "    accuracy                           0.75      4040\n",
            "   macro avg       0.76      0.75      0.75      4040\n",
            "weighted avg       0.76      0.75      0.75      4040\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpD7Lw0vdmvL"
      },
      "source": [
        "# ***GRAPHS***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ovg_lFpmPrHP",
        "outputId": "51253722-f39e-4161-e2a7-ceffdaa81b36"
      },
      "source": [
        "#Train-Val Loss Graph\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "loss_train = train_loss\n",
        "loss_val = validation_loss\n",
        "epochs = range(1,len(train_loss)+1)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Ke8bU-oP8nH0",
        "outputId": "5df4aacc-c4f0-4ec5-d40c-53deafb4ecca"
      },
      "source": [
        "#Accuracy\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "accuracy = accuracy_val\n",
        "epochs = range(1, len(accuracy)+1)\n",
        "plt.plot(epochs, accuracy, 'g', label='Accuracy')\n",
        "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vIrwPiQxdds"
      },
      "source": [
        "# ***Testing***"
      ]
    }
  ]
}